{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "#only for calculating mode\n",
    "from scipy import stats as st\n",
    "\n",
    "#digits from sci-kit learn\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading datasets. Digits requires some additional pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parkinsons dataset\n",
    "parkinsons = pd.read_csv('./datasets/parkinsons.csv')\n",
    "parkinsons.rename(columns={'Diagnosis': 'class'}, inplace=True)\n",
    "\n",
    "#titanic dataset\n",
    "titanic = pd.read_csv('./datasets/titanic_processed.csv')\n",
    "\n",
    "#loan\n",
    "loan = pd.read_csv('./datasets/loan.csv')\n",
    "loan = loan.drop(columns=['Loan_ID'])\n",
    "#I have to map every single column to numbers - taken from RF algorithm\n",
    "loan = loan.rename(columns={'Loan_Status': 'class'})\n",
    "loan['class'] = loan['class'].map({'Y': 1, 'N': 0})\n",
    "loan['Gender'] = loan['Gender'].map({'Male': 1, 'Female': 0})\n",
    "loan['Married'] = loan['Married'].map({'Yes': 1, 'No': 0})\n",
    "loan['Dependents'] = loan['Dependents'].map({'3+': 3, '2':2, '1':1, '0':0})\n",
    "loan['Education'] = loan['Education'].map({'Graduate': 1, 'Not Graduate': 0})\n",
    "loan['Self_Employed'] = loan['Self_Employed'].map({'Yes': 1, 'No': 0})\n",
    "loan['Property_Area'] = loan['Property_Area'].map({'Urban': 2, 'Semiurban': 1, 'Rural':0})\n",
    "\n",
    "#digits from sci-kit learn\n",
    "def load_digits():\n",
    "    dig = datasets.load_digits(return_X_y=True)\n",
    "\n",
    "    X_df = pd.DataFrame(dig[0])\n",
    "    y_df = pd.DataFrame(dig[1], columns=['class'])\n",
    "    \n",
    "    dig = pd.concat([X_df, y_df], axis=1)\n",
    "    return dig\n",
    "\n",
    "digits = load_digits()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Globals are used as hyper-parameters in very specific places. Each dataset/example test should reset these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL_MODEL = False\n",
    "def reset_hyperparameters():\n",
    "    global LAMBDA, ALPHA\n",
    "    LAMBDA = 0\n",
    "    ALPHA = 1\n",
    "    \n",
    "reset_hyperparameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing data\n",
    "- cross-validation with stratified sampling\n",
    "- normalization of data\n",
    "- one-hot encoding of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_training_data(dataset):\n",
    "    \"\"\"\n",
    "    Normalizes non-class columns to the [0,1] range\n",
    "    returns non-class columns\n",
    "    \"\"\"\n",
    "    for j in range(dataset.shape[1]):\n",
    "        max = np.max(dataset[:,j])\n",
    "        min = np.min(dataset[:,j])\n",
    "        \n",
    "        if (min - max != 0):\n",
    "            dataset[:,j] = (dataset[:,j] - min)/(max-min)\n",
    "        else:\n",
    "            dataset[:,j] = 0\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def vectorize_classes(class_labels):\n",
    "    \"\"\"\n",
    "    one-hot encoding of class labels\n",
    "    - returns: list of vectors representing y\n",
    "    \"\"\"\n",
    "    classes = np.unique(class_labels)\n",
    "    classes = np.sort(classes)\n",
    "    \n",
    "    #first index is index of second class in codified data\n",
    "    num_classes = len(classes)\n",
    "    index_col = np.arange(0, num_classes)\n",
    "    classes = np.c_[index_col, classes]\n",
    "    \n",
    "    y = []\n",
    "    \n",
    "    for i in range(class_labels.shape[0]):\n",
    "        curr_class = class_labels[i]\n",
    "        class_index = np.where(classes[:, 1] == curr_class)[0][0]\n",
    "        \n",
    "        # one hot encoding\n",
    "        \n",
    "        #reshapes into vector\n",
    "        y_i = np.zeros((num_classes), dtype=np.int64)\n",
    "        \n",
    "        y_i[class_index] = 1\n",
    "        y.append(y_i)\n",
    "    \n",
    "    #converts to n by num_classes matrix\n",
    "    y = np.vstack(y)\n",
    "\n",
    "    return y\n",
    "\n",
    "def create_folds(x, y, num_folds):\n",
    "    \"\"\"\n",
    "    creates arrays of sub datasets used for cross-validation\n",
    "    - x and y have to be treated differently\n",
    "    \"\"\"\n",
    "    x_folds = []\n",
    "    y_folds = []\n",
    "    \n",
    "    #roughly the number of observations per class\n",
    "    fold_size = x.shape[0] / num_folds\n",
    "    \n",
    "    #folds have correct distribution of classes\n",
    "    for i in range(num_folds):\n",
    "        #prevents uneven distribution of fold-sizes\n",
    "        lower = i * fold_size\n",
    "        if (i < num_folds - 1):\n",
    "            upper = (i + 1) * fold_size\n",
    "        else:\n",
    "            upper = x.shape[0]\n",
    "        \n",
    "        x_fold = x[int(lower):int(upper)]\n",
    "        y_fold = y[int(lower):int(upper)]\n",
    "        \n",
    "        x_folds.append(x_fold)\n",
    "        y_folds.append(y_fold)\n",
    "        \n",
    "    return x_folds, y_folds\n",
    "    \n",
    "def prepare_data(dataset, num_folds):\n",
    "    #shuffling data - this is the only time we do so\n",
    "    dataset = dataset.sample(frac=1)\n",
    "    \n",
    "    #separates class_labels\n",
    "    class_labels = dataset.pop('class')\n",
    "    \n",
    "    #converts to numpy matrix\n",
    "    x = dataset.to_numpy()\n",
    "    class_labels = class_labels.to_numpy()\n",
    "    \n",
    "    #normalizes x data to [0,1] range\n",
    "    #this prevents overflow errors and helps processing\n",
    "    x = normalize_training_data(x)\n",
    "    \n",
    "    #encodes classes into vectors (one hot encoding)\n",
    "    y = vectorize_classes(class_labels)\n",
    "    \n",
    "    #creates an array of sub-datasets used later for cross-validation\n",
    "    x_folds, y_folds = create_folds(x, y, num_folds)\n",
    "\n",
    "    return x_folds, y_folds\n",
    "    \n",
    "def create_sets(x, y, curr_fold):\n",
    "    \"\"\"\n",
    "    Returns train and test sets for cross-validation\n",
    "    - Converts x data frames into numpy matrix\n",
    "    \"\"\"\n",
    "    x = copy.deepcopy(x)\n",
    "    y = copy.deepcopy(y)\n",
    "    \n",
    "    #test data, we pop one fold\n",
    "    test_x = x.pop(curr_fold)\n",
    "    \n",
    "    test_y = y.pop(curr_fold)\n",
    "    \n",
    "    #train data\n",
    "    train_x = np.vstack(x)\n",
    "    train_y = np.vstack(y)\n",
    "    \n",
    "    return train_x, train_y, test_x, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(dimensions, gradient:bool):\n",
    "    \"\"\"\n",
    "    Initializes weights\n",
    "    \"\"\"\n",
    "    matrices = []\n",
    "    for i in range(len(dimensions) - 1):\n",
    "\n",
    "        if gradient:\n",
    "            weights = np.zeros((dimensions[i+1], dimensions[i]+1))\n",
    "        else:\n",
    "            #samples initial theta matrices from a standard normal distribution\n",
    "            weights = np.random.randn(dimensions[i+1], dimensions[i]+1)\n",
    "            \n",
    "        matrices.append(weights)\n",
    "    return matrices\n",
    "\n",
    "def forward_propagation(instance, weights_list):\n",
    "    \"\"\"\n",
    "    Forward propagates a training instance \n",
    "        returns final layer of network\n",
    "                and all layers of neural network\n",
    "    \"\"\"\n",
    "    activated_layers = []\n",
    "    \n",
    "    #activates input layer\n",
    "    # act_layer = sigmoid(instance)\n",
    "    act_layer = np.array(instance)\n",
    "    \n",
    "    #reshapes into a vector\n",
    "    act_layer = act_layer.reshape(-1,1)\n",
    "    \n",
    "    #adds a bias term\n",
    "    act_layer = np.insert(act_layer, 0, values=1)\n",
    "    \n",
    "    #reshapes again after insertion\n",
    "    act_layer = act_layer.reshape(-1,1)\n",
    "    \n",
    "    #appends\n",
    "    activated_layers.append(act_layer)\n",
    "    \n",
    "    for weights in weights_list:\n",
    "        layer = np.dot(weights, act_layer)\n",
    "        \n",
    "        #sigmoid function layer\n",
    "        act_layer = sigmoid(layer)\n",
    "        \n",
    "        #adding bias for next phase\n",
    "        act_layer = np.insert(act_layer, 0, values=1)   \n",
    "        \n",
    "        #reshapes after insertion\n",
    "        act_layer = act_layer.reshape(-1,1)\n",
    "        \n",
    "        #store the activated layer for back-propagation\n",
    "        activated_layers.append(act_layer)\n",
    "        \n",
    "\n",
    "    y_hat = act_layer[1:]\n",
    "    activated_layers[len(activated_layers)-1] = activated_layers[len(activated_layers)-1][1:]\n",
    "    \n",
    "    return y_hat, activated_layers\n",
    "\n",
    "def sigmoid(vector):\n",
    "    return 1 / (1 + np.exp(-vector))\n",
    "\n",
    "def compute_cost(y_hat, y_true):\n",
    "    #dimensions must match\n",
    "    cost_arr = - y_true * np.log(y_hat) - (1-y_true) * np.log(1-y_hat)\n",
    "    #common overflow error\n",
    "    return np.sum(cost_arr)\n",
    "\n",
    "def regularize_cost(weights_list, batch_size):\n",
    "    \"\"\"\n",
    "    Adds cost for overfitting\n",
    "        Computes squared sum of all non-bias weights\n",
    "    \"\"\"\n",
    "    sum = 0\n",
    "    for weights in weights_list:\n",
    "        weights = weights[:, 1:]\n",
    "        weights = np.square(weights)\n",
    "        sum += np.sum(weights)\n",
    "    \n",
    "    sum = LAMBDA * sum / (2 * batch_size) \n",
    "    return sum\n",
    "\n",
    "def compute_delta(weights_list, activation_layers, y_true):\n",
    "    \"\"\"\n",
    "    Computes delta vectors for each hidden layer\n",
    "    \"\"\"\n",
    "    #we remove from the end and exclude the first layer\n",
    "    #we also pop to exclude the final pred. layer\n",
    "    \n",
    "    #delta of prediction layer\n",
    "    delta_next = activation_layers.pop() \n",
    "    delta_next = delta_next - y_true.reshape(-1,1)\n",
    "    \n",
    "    #stores deltas in the same order as activation layers\n",
    "    deltas = [delta_next]\n",
    "    \n",
    "    while len(activation_layers) > 1:\n",
    "        thetas = weights_list.pop()\n",
    "        activation = activation_layers.pop()\n",
    "        activation_2 = 1 - activation\n",
    "        \n",
    "        delta_curr = np.dot(np.transpose(thetas), delta_next)\n",
    "        delta_curr = delta_curr * activation * activation_2\n",
    "        \n",
    "        #removes delta associated with bias node\n",
    "        delta_curr = delta_curr[1:]\n",
    "        \n",
    "        #reshapes into a vector\n",
    "        # delta_curr = delta_curr.reshape(-1,1)\n",
    "        \n",
    "        #stores and resets next delta layer\n",
    "        deltas.insert(0,delta_curr)\n",
    "        delta_next = delta_curr\n",
    "        \n",
    "    return deltas\n",
    "\n",
    "def compute_gradients(activation_layers, deltas, previous_gradients):\n",
    "    \"\"\"\n",
    "    - computes gradients based on activations of neurons and next layer deltas\n",
    "    - adds these to the previous gradients\n",
    "    \"\"\"\n",
    "    #deltas list starts at first hidden layer (second layer), \n",
    "    for i in range(len(activation_layers) - 1):\n",
    "        activation = np.transpose(activation_layers[i])\n",
    "        delta_t = deltas[i]\n",
    "        \n",
    "        gradient_layer = np.dot(delta_t, activation)\n",
    "\n",
    "        #accumulates to previous gradients\n",
    "        previous_gradients[i] += gradient_layer\n",
    "        \n",
    "    return previous_gradients\n",
    "\n",
    "def regularize_gradients(weights_list, prev_gradients, batch_size):\n",
    "    \"\"\"\n",
    "    Computes a regularized gradient which punishes large weights\n",
    "    - adds it to the previous gradient and normalizes it for final updates\n",
    "    \"\"\"\n",
    "    # regularization if any\n",
    "    reg_gradients = []\n",
    "    for weights in weights_list:\n",
    "        reg_gradient = LAMBDA * copy.deepcopy(weights)\n",
    "        reg_gradient[:,0] = 0\n",
    "        reg_gradients.append(reg_gradient)\n",
    "    \n",
    "    for i in range(len(prev_gradients)):\n",
    "        reg_gradients[i] += prev_gradients[i]\n",
    "        \n",
    "    for i in range(len(reg_gradients)):\n",
    "        reg_gradients[i] = reg_gradients[i] / batch_size\n",
    "    \n",
    "    return reg_gradients\n",
    "\n",
    "def update_weights(weights_list, reg_gradients):\n",
    "    \"\"\"\n",
    "    Updates weights based on regularized gradients\n",
    "    \"\"\"\n",
    "    for i in range(len(weights_list)):\n",
    "        weights_list[i] = weights_list[i] - ALPHA * reg_gradients[i]\n",
    "    \n",
    "    return weights_list\n",
    "\n",
    "def record_confusion(output):\n",
    "    \"\"\"\n",
    "    Compiles accuracy, recall, precision, and f1 score based on output\n",
    "    \"\"\"\n",
    "    acc = 0\n",
    "    prec = 0\n",
    "    rec = 0\n",
    "    num_classes = output.shape[0]\n",
    "    n = np.sum(output[:,1])\n",
    "    \n",
    "    #for each class compute performance metrics\n",
    "    for c in range(num_classes):\n",
    "        tp, tn, fp, fn = 0,0,0,0\n",
    "        \n",
    "        pred_pos = output[c,1]\n",
    "        act_pos = output[c,2]\n",
    "        \n",
    "        pred_neg = np.sum(output[:,1]) - pred_pos\n",
    "        act_neg = np.sum(output[:,2]) - act_pos\n",
    "        \n",
    "        tp = pred_pos if pred_pos <= act_pos else act_pos\n",
    "        tn = pred_neg if pred_neg <= act_neg else act_neg\n",
    "        fn = act_pos - tp\n",
    "        fp = act_neg - tn\n",
    "        \n",
    "        acc += (tp + tn) / n\n",
    "        \n",
    "        #precision causes errors: division by zero\n",
    "        if tp + fp == 0:\n",
    "            prec += 0\n",
    "        elif tp != 0:\n",
    "            prec += tp / (tp + fp)\n",
    "        else:\n",
    "            prec += 0\n",
    "            \n",
    "        #found out later that recall also causes errors\n",
    "        rec += tp / (tp + fn) if tp != 0 else 0\n",
    "    \n",
    "    acc = acc / num_classes\n",
    "    prec = prec / num_classes\n",
    "    rec = rec / num_classes\n",
    "    \n",
    "    f1 = 2 * prec * rec / (prec + rec) if prec + rec != 0 else 0\n",
    "    \n",
    "    ret = {\n",
    "        'accuracy':acc,\n",
    "        'precision':prec,\n",
    "        'recall':rec,\n",
    "        'f1-score':f1\n",
    "    } \n",
    "    return ret\n",
    "    \n",
    "def neural_network(dataset, architecture, num_folds, batch_size, num_iterations):\n",
    "    \"\"\"\n",
    "    Runs a neural network based on a dataset, architecture\n",
    "    - Cross validation is used for training with num_folds\n",
    "    \"\"\"\n",
    "    x,y = prepare_data(dataset, num_folds)\n",
    "    metrics_sum = []\n",
    "\n",
    "    for curr_fold in range(num_folds):\n",
    "        train_X, train_y, test_X, test_y = create_sets(\n",
    "            x, y, curr_fold)\n",
    "        \n",
    "        column_weights = []\n",
    "        print('fold ' + str(curr_fold))\n",
    "        for j in range(train_X.shape[1]):\n",
    "            train_x = train_X[:,j]\n",
    "        \n",
    "            weights_list = initialize_weights(architecture, gradient=False)\n",
    "            empty_gradient = initialize_weights(architecture, gradient=True)\n",
    "            \n",
    "            gradients = copy.deepcopy(empty_gradient)\n",
    "            \n",
    "            cost_list = np.array([])\n",
    "            cost = 0\n",
    "            \n",
    "            instances_seen = 0\n",
    "            \n",
    "            #only for the final model\n",
    "            if FINAL_MODEL:\n",
    "                test_costs = []\n",
    "            \n",
    "            #stopping criteria:\n",
    "                #loops num_iterations times over the dataset\n",
    "            for _ in range(num_iterations):\n",
    "            \n",
    "                #loops over every iteration of dataset, updates weights when i is a multiple of batch_size\n",
    "                for i in range(train_x.shape[0]): \n",
    "                    x_i = train_x[i].reshape(-1,1)\n",
    "                    y_i = train_y[i].reshape(-1,1)\n",
    "                    \n",
    "                    #forward propagation\n",
    "                    y_hat, activation_layers = forward_propagation(x_i, weights_list)\n",
    "                    \n",
    "                    #adds cost for this training instance\n",
    "                    cost += compute_cost(y_hat, y_i)\n",
    "\n",
    "                    #backward pass\n",
    "                    deltas = compute_delta(weights_list.copy(), activation_layers.copy(), y_i)\n",
    "                    \n",
    "                    gradients = compute_gradients(activation_layers, deltas, gradients)\n",
    "                    \n",
    "                    #updates for the following conditional\n",
    "                    instances_seen += 1\n",
    "                    \n",
    "                    #updates weights based on regularized gradients\n",
    "                    if (instances_seen % batch_size) == 0:\n",
    "                        #regularizes cost and records:\n",
    "                        cost = cost / batch_size\n",
    "                        cost += regularize_cost(weights_list, batch_size)\n",
    "                        cost_list = np.append(cost_list, cost)\n",
    "                        #resets cost\n",
    "                        cost = 0\n",
    "                        \n",
    "                        #regularizes gradients\n",
    "                        reg_gradients = regularize_gradients(weights_list, gradients, batch_size)\n",
    "                        gradients = copy.deepcopy(empty_gradient)\n",
    "                    \n",
    "                        #updates weights\n",
    "                        weights_list = update_weights(weights_list, reg_gradients)\n",
    "                        \n",
    "        column_weights.append(weights_list)         \n",
    "                    \n",
    "        print('evaluation and ensemble')\n",
    "        \n",
    "        #first column is class, second is y_hat, third is true\n",
    "        output = np.c_[np.arange(0, test_y.shape[1]), np.zeros(test_y.shape[1]), np.zeros(test_y.shape[1])]\n",
    "        \n",
    "        outs = []\n",
    "        for j in range(train_X.shape[1]):\n",
    "            out = []\n",
    "            #forward propagation of test set\n",
    "            for i in range(test_X.shape[0]):\n",
    "                x_i = test_X[i,j].reshape(-1,1)\n",
    "                y_i = test_y[i].reshape(-1,1)\n",
    "                \n",
    "                y_hat, activation_layers = forward_propagation(x_i, weights_list)\n",
    "                \n",
    "                y_hat_index = np.argmax(y_hat)\n",
    "                \n",
    "                out.append(y_hat_index)\n",
    "                \n",
    "            outs.append(out)\n",
    "            y_true_index = np.argmax(y_i)\n",
    "            output[y_true_index, 2] += 1 \n",
    "            \n",
    "        outs = np.array(outs)\n",
    "        \n",
    "        out = []\n",
    "        for j in range(outs.shape[1]):\n",
    "            mode = st.mode(outs[:,j])\n",
    "            mode = np.unique(mode[0])\n",
    "            \n",
    "            indecies = []\n",
    "            \n",
    "            mode_indices = np.where(np.isin(outs[:, j], mode))[0]\n",
    "        \n",
    "            x_i = test_X[j, mode_indices]\n",
    "            \n",
    "            train_x = train_X[:,mode_indices]\n",
    "            \n",
    "            distance = (train_x - x_i) ** 2\n",
    "            \n",
    "            distances = np.sum(distance, axis=1)\n",
    "            \n",
    "            #indecies\n",
    "            \n",
    "            distances = np.argsort(distances)\n",
    "            \n",
    "            #25% of the closest\n",
    "            closest = distances[:int(0.25 * distances.shape[0])]\n",
    "            \n",
    "            #gets vote\n",
    "            \n",
    "            closest_y = train_y[closest,:]\n",
    "            \n",
    "            closest_y = np.argmax(closest_y, axis=1)\n",
    "            \n",
    "            out.append(st.mode(closest_y)[0][0])\n",
    "            \n",
    "        outs = np.r_[outs, np.array(out).reshape(1,-1)]\n",
    "        \n",
    "        outs = np.apply_along_axis(lambda x: st.mode(x), axis=1, arr=outs)\n",
    "        \n",
    "        out = []\n",
    "        \n",
    "        for i in range(outs.shape[0]):\n",
    "            out.append(outs[i][0][0])\n",
    "        \n",
    "        out = np.array(out)\n",
    "        \n",
    "        for o in out:\n",
    "            output[o, 1] += 1\n",
    "\n",
    "        #evaluates and records metrics\n",
    "        scores = record_confusion(output)\n",
    "        \n",
    "        metrics_sum.append(scores)\n",
    "        \n",
    "    #compiles metrics and model hyperparameters\n",
    "    stats = {\n",
    "        'accuracy': [m['accuracy'] for m in metrics_sum],\n",
    "        'precision': [m['precision'] for m in metrics_sum],\n",
    "        'recall': [m['recall'] for m in metrics_sum],\n",
    "        'f1-score': [m['f1-score'] for m in metrics_sum],\n",
    "        'alpha': ALPHA,\n",
    "        'lambda': LAMBDA,\n",
    "        'num_iterations': num_iterations,\n",
    "        'batch_size': batch_size\n",
    "    }\n",
    "\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates graphs/records for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress_metrics(stats, log=False):\n",
    "    acc = np.mean(stats['accuracy'])\n",
    "    rec = np.mean(stats['recall'])\n",
    "    prec = np.mean(stats['precision'])\n",
    "    f1 = np.mean(stats['f1-score'])\n",
    "    \n",
    "    \n",
    "    if log:\n",
    "        print(f'accuracy: {acc:.4f}')\n",
    "        print(f'recall: {rec:.4f}')\n",
    "        print(f'precision: {prec:.4f}')\n",
    "        print(f'F1-Score: {f1:.4f}')\n",
    "    \n",
    "    ret = {\n",
    "        'm_accuracy':acc,\n",
    "        'm_recall':rec,\n",
    "        'm_precision':prec,\n",
    "        'm_f1-score':f1,\n",
    "    }\n",
    "    stats.update(ret)\n",
    "    return\n",
    "\n",
    "def summarize_to_latex(model_data):    \n",
    "    data = {}\n",
    "    \n",
    "    \n",
    "    for i, m in zip(range(len(model_data)), model_data):\n",
    "        one = round(m['alpha'], 3)\n",
    "        two = round(m['lambda'], 4)\n",
    "        four = round(m['m_accuracy'], 4)\n",
    "        five = round(m['m_f1-score'], 4)\n",
    "        six = m['num_iterations']\n",
    "        seven = m['batch_size']\n",
    "        \n",
    "        data[f'Model {i+1}'] = [one, two, four, five, six, seven]\n",
    "    \n",
    "    #seven columns\n",
    "    metrics = ['Learning Rate $\\\\alpha$', 'Regularization $\\\\lambda$', 'Mean Accuracy',\n",
    "               'Mean F1-score', 'Number of Iterations', 'Batch Size']\n",
    "    \n",
    "    df = pd.DataFrame(data, index=metrics)\n",
    "\n",
    "    latex_table = df.to_latex(float_format=\"%.4f\")\n",
    "\n",
    "    print(latex_table)\n",
    "\n",
    "def accuracy_spread(stats, title=''):\n",
    "    acc = stats['accuracy']\n",
    "    mean_acc = stats['m_accuracy']\n",
    "    \n",
    "    plt.boxplot(acc, vert=False)\n",
    "    plt.title('Accuracy Spread for ' + title)\n",
    "    plt.xlabel('Accuracy')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    plt.axvline(x=mean_acc, color='orange', linestyle='-', label=f'mean = {mean_acc}')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.savefig(f'./figures/{title}_variant.png')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each dataset, I visually confirm that the training cost flatlines\n",
    "- I train each model individually and by hand to try and achieve the highest results, recording what I've done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALPHA = 0.1\n",
    "# LAMBDA = 0\n",
    "# model = neural_network(digits, architecture=[1, 1, 10], num_folds=10, batch_size=10, num_iterations=20)\n",
    "\n",
    "# compress_metrics(model, log=True)\n",
    "\n",
    "# summarize_to_latex(model_data=[model])\n",
    "\n",
    "# accuracy_spread(model, title='Digits dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0\n",
      "evaluation and ensemble\n",
      "fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y3/8hh0nl3j6qgdl0lq3v_dj7k00000gn/T/ipykernel_75738/2844400968.py:316: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode = st.mode(outs[:,j])\n",
      "/var/folders/y3/8hh0nl3j6qgdl0lq3v_dj7k00000gn/T/ipykernel_75738/2844400968.py:344: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  out.append(st.mode(closest_y)[0][0])\n",
      "/var/folders/y3/8hh0nl3j6qgdl0lq3v_dj7k00000gn/T/ipykernel_75738/2844400968.py:348: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  outs = np.apply_along_axis(lambda x: st.mode(x), axis=1, arr=outs)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m ALPHA \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m\n\u001b[1;32m      2\u001b[0m LAMBDA \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mneural_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtitanic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marchitecture\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_folds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m compress_metrics(model, log\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      7\u001b[0m summarize_to_latex(model_data\u001b[38;5;241m=\u001b[39m[model])\n",
      "Cell \u001b[0;32mIn[5], line 261\u001b[0m, in \u001b[0;36mneural_network\u001b[0;34m(dataset, architecture, num_folds, batch_size, num_iterations)\u001b[0m\n\u001b[1;32m    258\u001b[0m y_hat, activation_layers \u001b[38;5;241m=\u001b[39m forward_propagation(x_i, weights_list)\n\u001b[1;32m    260\u001b[0m \u001b[38;5;66;03m#adds cost for this training instance\u001b[39;00m\n\u001b[0;32m--> 261\u001b[0m cost \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mcompute_cost\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_hat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_i\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m#backward pass\u001b[39;00m\n\u001b[1;32m    264\u001b[0m deltas \u001b[38;5;241m=\u001b[39m compute_delta(weights_list\u001b[38;5;241m.\u001b[39mcopy(), activation_layers\u001b[38;5;241m.\u001b[39mcopy(), y_i)\n",
      "Cell \u001b[0;32mIn[5], line 67\u001b[0m, in \u001b[0;36mcompute_cost\u001b[0;34m(y_hat, y_true)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_cost\u001b[39m(y_hat, y_true):\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;66;03m#dimensions must match\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m     cost_arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m y_true \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(y_hat) \u001b[38;5;241m-\u001b[39m (\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39my_true) \u001b[38;5;241m*\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43my_hat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m#common overflow error\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39msum(cost_arr)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ALPHA = 0.1\n",
    "LAMBDA = 0\n",
    "model = neural_network(titanic, architecture=[1, 1, 2], num_folds=10, batch_size=10, num_iterations=100)\n",
    "\n",
    "compress_metrics(model, log=True)\n",
    "\n",
    "summarize_to_latex(model_data=[model])\n",
    "\n",
    "accuracy_spread(model, title='Titanic Dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHA = 0.1\n",
    "LAMBDA = 0\n",
    "model = neural_network(loan, architecture=[1, 1, 2], num_folds=10, batch_size=10, num_iterations=100)\n",
    "\n",
    "compress_metrics(model, log=True)\n",
    "\n",
    "summarize_to_latex(model_data=[model])\n",
    "\n",
    "accuracy_spread(model, title='Loans Dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHA = 0.1\n",
    "LAMBDA = 0\n",
    "model = neural_network(parkinsons, architecture=[1, 1, 2], num_folds=10, batch_size=10, num_iterations=100)\n",
    "\n",
    "compress_metrics(model, log=True)\n",
    "\n",
    "summarize_to_latex(model_data=[model])\n",
    "\n",
    "accuracy_spread(model, title='Parkinson\\'s Dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [\n",
    "    [2,3,4,5],\n",
    "    [2,3,4,5],\n",
    "    [6,7,8,9],\n",
    "    [6,7,8,9]\n",
    "]\n",
    "test = np.array(test)\n",
    "\n",
    "test_mode = st.mode(test, axis=1)\n",
    "\n",
    "print(test_mode)\n",
    "print(test_mode[0])\n",
    "print(test_mode[0][0])\n",
    "print(test_mode[0][0][0])\n",
    "print()\n",
    "\n",
    "unique_modes = np.unique(test_mode[0])\n",
    "print(unique_modes)\n",
    "print(unique_modes[1])\n",
    "\n",
    "\n",
    "# print(np.isin(test[:, 0]))\n",
    "mode_indices = np.where(np.isin(test[:, 0], unique_modes))[0]\n",
    "print(mode_indices)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
