{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading datasets. For some reason (maybe lack of memory) these are just forgotten by the environment. So reports may be needed later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine = pd.read_csv('datasets/wine.csv', sep='\\s+')\n",
    "house = pd.read_csv('datasets/house_votes_84.csv')\n",
    "cancer = pd.read_csv('datasets/cancer.csv', sep='\\s+')\n",
    "contraceptive = pd.read_csv('datasets/contraceptive+method+choice/cmc.data', header=None)\n",
    "\n",
    "#renames class column so my network can find output:\n",
    "cancer.rename(columns={'Class': 'class'}, inplace=True)\n",
    "contraceptive.rename(columns={9: 'class'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Globals are used as hyper-parameters in very specific places. Each dataset/example test should reset these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL_MODEL = False\n",
    "def reset_hyperparameters():\n",
    "    global LAMBDA, ALPHA\n",
    "    LAMBDA = 0\n",
    "    ALPHA = 1\n",
    "    \n",
    "reset_hyperparameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing data\n",
    "- cross-validation with stratified sampling\n",
    "- normalization of data\n",
    "- one-hot encoding of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_training_data(dataset):\n",
    "    \"\"\"\n",
    "    Normalizes non-class columns to the [0,1] range\n",
    "    returns non-class columns\n",
    "    \"\"\"\n",
    "    for j in range(dataset.shape[1]):\n",
    "        max = np.max(dataset[:,j])\n",
    "        min = np.min(dataset[:,j])\n",
    "        \n",
    "        dataset[:,j] = (dataset[:,j] - min)/(max-min)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def vectorize_classes(class_labels):\n",
    "    \"\"\"\n",
    "    one-hot encoding of class labels\n",
    "    - returns: list of vectors representing y\n",
    "    \"\"\"\n",
    "    classes = np.unique(class_labels)\n",
    "    classes = np.sort(classes)\n",
    "    \n",
    "    #first index is index of second class in codified data\n",
    "    num_classes = len(classes)\n",
    "    index_col = np.arange(0, num_classes)\n",
    "    classes = np.c_[index_col, classes]\n",
    "    \n",
    "    y = []\n",
    "    \n",
    "    for i in range(class_labels.shape[0]):\n",
    "        curr_class = class_labels[i]\n",
    "        class_index = np.where(classes[:, 1] == curr_class)[0][0]\n",
    "        \n",
    "        # one hot encoding\n",
    "        \n",
    "        #reshapes into vector\n",
    "        y_i = np.zeros((num_classes), dtype=np.int64)\n",
    "        \n",
    "        y_i[class_index] = 1\n",
    "        y.append(y_i)\n",
    "    \n",
    "    #converts to n by num_classes matrix\n",
    "    y = np.vstack(y)\n",
    "\n",
    "    return y\n",
    "\n",
    "def create_folds(x, y, num_folds):\n",
    "    \"\"\"\n",
    "    creates arrays of sub datasets used for cross-validation\n",
    "    - x and y have to be treated differently\n",
    "    \"\"\"\n",
    "    x_folds = []\n",
    "    y_folds = []\n",
    "    \n",
    "    #roughly the number of observations per class\n",
    "    fold_size = x.shape[0] / num_folds\n",
    "    \n",
    "    #folds have correct distribution of classes\n",
    "    for i in range(num_folds):\n",
    "        #prevents uneven distribution of fold-sizes\n",
    "        lower = i * fold_size\n",
    "        if (i < num_folds - 1):\n",
    "            upper = (i + 1) * fold_size\n",
    "        else:\n",
    "            upper = x.shape[0]\n",
    "        \n",
    "        x_fold = x[int(lower):int(upper)]\n",
    "        y_fold = y[int(lower):int(upper)]\n",
    "        \n",
    "        x_folds.append(x_fold)\n",
    "        y_folds.append(y_fold)\n",
    "        \n",
    "    return x_folds, y_folds\n",
    "    \n",
    "def prepare_data(dataset, num_folds):\n",
    "    #shuffling data - this is the only time we do so\n",
    "    dataset = dataset.sample(frac=1)\n",
    "    \n",
    "    #separates class_labels\n",
    "    class_labels = dataset.pop('class')\n",
    "    \n",
    "    #converts to numpy matrix\n",
    "    x = dataset.to_numpy()\n",
    "    class_labels = class_labels.to_numpy()\n",
    "    \n",
    "    #normalizes x data to [0,1] range\n",
    "    #this prevents overflow errors and helps processing\n",
    "    x = normalize_training_data(x)\n",
    "    \n",
    "    #encodes classes into vectors (one hot encoding)\n",
    "    y = vectorize_classes(class_labels)\n",
    "    \n",
    "    #creates an array of sub-datasets used later for cross-validation\n",
    "    x_folds, y_folds = create_folds(x, y, num_folds)\n",
    "\n",
    "    return x_folds, y_folds\n",
    "    \n",
    "def create_sets(x, y, curr_fold):\n",
    "    \"\"\"\n",
    "    Returns train and test sets for cross-validation\n",
    "    - Converts x data frames into numpy matrix\n",
    "    \"\"\"\n",
    "    x = copy.deepcopy(x)\n",
    "    y = copy.deepcopy(y)\n",
    "    \n",
    "    #test data, we pop one fold\n",
    "    test_x = x.pop(curr_fold)\n",
    "    \n",
    "    test_y = y.pop(curr_fold)\n",
    "    \n",
    "    #train data\n",
    "    train_x = np.vstack(x)\n",
    "    train_y = np.vstack(y)\n",
    "    \n",
    "    return train_x, train_y, test_x, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(dimensions, gradient:bool):\n",
    "    \"\"\"\n",
    "    Initializes weights\n",
    "    \"\"\"\n",
    "    matrices = []\n",
    "    for i in range(len(dimensions) - 1):\n",
    "\n",
    "        if gradient:\n",
    "            weights = np.zeros((dimensions[i+1], dimensions[i]+1))\n",
    "        else:\n",
    "            #samples initial theta matrices from a standard normal distribution\n",
    "            weights = np.random.randn(dimensions[i+1], dimensions[i]+1)\n",
    "            \n",
    "        matrices.append(weights)\n",
    "    return matrices\n",
    "\n",
    "def forward_propagation(instance, weights_list):\n",
    "    \"\"\"\n",
    "    Forward propagates a training instance \n",
    "        returns final layer of network\n",
    "                and all layers of neural network\n",
    "    \"\"\"\n",
    "    activated_layers = []\n",
    "    \n",
    "    #activates input layer\n",
    "    # act_layer = sigmoid(instance)\n",
    "    act_layer = np.array(instance)\n",
    "    \n",
    "    #reshapes into a vector\n",
    "    act_layer = act_layer.reshape(-1,1)\n",
    "    \n",
    "    #adds a bias term\n",
    "    act_layer = np.insert(act_layer, 0, values=1)\n",
    "    \n",
    "    #reshapes again after insertion\n",
    "    act_layer = act_layer.reshape(-1,1)\n",
    "    \n",
    "    #appends\n",
    "    activated_layers.append(act_layer)\n",
    "    \n",
    "    for weights in weights_list:\n",
    "        layer = np.dot(weights, act_layer)\n",
    "        \n",
    "        #sigmoid function layer\n",
    "        act_layer = sigmoid(layer)\n",
    "        \n",
    "        #adding bias for next phase\n",
    "        act_layer = np.insert(act_layer, 0, values=1)   \n",
    "        \n",
    "        #reshapes after insertion\n",
    "        act_layer = act_layer.reshape(-1,1)\n",
    "        \n",
    "        #store the activated layer for back-propagation\n",
    "        activated_layers.append(act_layer)\n",
    "        \n",
    "\n",
    "    y_hat = act_layer[1:]\n",
    "    activated_layers[len(activated_layers)-1] = activated_layers[len(activated_layers)-1][1:]\n",
    "    \n",
    "    return y_hat, activated_layers\n",
    "\n",
    "def sigmoid(vector):\n",
    "    return 1 / (1 + np.exp(-vector))\n",
    "\n",
    "def compute_cost(y_hat, y_true):\n",
    "    #dimensions must match\n",
    "    cost_arr = - y_true * np.log(y_hat) - (1-y_true) * np.log(1-y_hat)\n",
    "    #common overflow error\n",
    "    return np.sum(cost_arr)\n",
    "\n",
    "def regularize_cost(weights_list, batch_size):\n",
    "    \"\"\"\n",
    "    Adds cost for overfitting\n",
    "        Computes squared sum of all non-bias weights\n",
    "    \"\"\"\n",
    "    sum = 0\n",
    "    for weights in weights_list:\n",
    "        weights = weights[:, 1:]\n",
    "        weights = np.square(weights)\n",
    "        sum += np.sum(weights)\n",
    "    \n",
    "    sum = LAMBDA * sum / (2 * batch_size) \n",
    "    return sum\n",
    "\n",
    "def compute_delta(weights_list, activation_layers, y_true):\n",
    "    \"\"\"\n",
    "    Computes delta vectors for each hidden layer\n",
    "    \"\"\"\n",
    "    #we remove from the end and exclude the first layer\n",
    "    #we also pop to exclude the final pred. layer\n",
    "    \n",
    "    #delta of prediction layer\n",
    "    delta_next = activation_layers.pop() \n",
    "    delta_next = delta_next - y_true.reshape(-1,1)\n",
    "    \n",
    "    #stores deltas in the same order as activation layers\n",
    "    deltas = [delta_next]\n",
    "    \n",
    "    while len(activation_layers) > 1:\n",
    "        thetas = weights_list.pop()\n",
    "        activation = activation_layers.pop()\n",
    "        activation_2 = 1 - activation\n",
    "        \n",
    "        delta_curr = np.dot(np.transpose(thetas), delta_next)\n",
    "        delta_curr = delta_curr * activation * activation_2\n",
    "        \n",
    "        #removes delta associated with bias node\n",
    "        delta_curr = delta_curr[1:]\n",
    "        \n",
    "        #reshapes into a vector\n",
    "        # delta_curr = delta_curr.reshape(-1,1)\n",
    "        \n",
    "        #stores and resets next delta layer\n",
    "        deltas.insert(0,delta_curr)\n",
    "        delta_next = delta_curr\n",
    "        \n",
    "    return deltas\n",
    "\n",
    "def compute_gradients(activation_layers, deltas, previous_gradients):\n",
    "    \"\"\"\n",
    "    - computes gradients based on activations of neurons and next layer deltas\n",
    "    - adds these to the previous gradients\n",
    "    \"\"\"\n",
    "    #deltas list starts at first hidden layer (second layer), \n",
    "    for i in range(len(activation_layers) - 1):\n",
    "        activation = np.transpose(activation_layers[i])\n",
    "        delta_t = deltas[i]\n",
    "        \n",
    "        gradient_layer = np.dot(delta_t, activation)\n",
    "\n",
    "        #accumulates to previous gradients\n",
    "        previous_gradients[i] += gradient_layer\n",
    "        \n",
    "    return previous_gradients\n",
    "\n",
    "def regularize_gradients(weights_list, prev_gradients, batch_size):\n",
    "    \"\"\"\n",
    "    Computes a regularized gradient which punishes large weights\n",
    "    - adds it to the previous gradient and normalizes it for final updates\n",
    "    \"\"\"\n",
    "    # regularization if any\n",
    "    reg_gradients = []\n",
    "    for weights in weights_list:\n",
    "        reg_gradient = LAMBDA * copy.deepcopy(weights)\n",
    "        reg_gradient[:,0] = 0\n",
    "        reg_gradients.append(reg_gradient)\n",
    "    \n",
    "    for i in range(len(prev_gradients)):\n",
    "        reg_gradients[i] += prev_gradients[i]\n",
    "        \n",
    "    for i in range(len(reg_gradients)):\n",
    "        reg_gradients[i] = reg_gradients[i] / batch_size\n",
    "    \n",
    "    return reg_gradients\n",
    "\n",
    "def update_weights(weights_list, reg_gradients):\n",
    "    \"\"\"\n",
    "    Updates weights based on regularized gradients\n",
    "    \"\"\"\n",
    "    for i in range(len(weights_list)):\n",
    "        weights_list[i] = weights_list[i] - ALPHA * reg_gradients[i]\n",
    "    \n",
    "    return weights_list\n",
    "\n",
    "def record_confusion(output):\n",
    "    \"\"\"\n",
    "    Compiles accuracy, recall, precision, and f1 score based on output\n",
    "    \"\"\"\n",
    "    acc = 0\n",
    "    prec = 0\n",
    "    rec = 0\n",
    "    num_classes = output.shape[0]\n",
    "    n = np.sum(output[:,1])\n",
    "    \n",
    "    #for each class compute performance metrics\n",
    "    for c in range(num_classes):\n",
    "        tp, tn, fp, fn = 0,0,0,0\n",
    "        \n",
    "        pred_pos = output[c,1]\n",
    "        act_pos = output[c,2]\n",
    "        \n",
    "        pred_neg = np.sum(output[:,1]) - pred_pos\n",
    "        act_neg = np.sum(output[:,2]) - act_pos\n",
    "        \n",
    "        tp = pred_pos if pred_pos <= act_pos else act_pos\n",
    "        tn = pred_neg if pred_neg <= act_neg else act_neg\n",
    "        fn = act_pos - tp\n",
    "        fp = act_neg - tn\n",
    "        \n",
    "        acc += (tp + tn) / n\n",
    "        \n",
    "        #precision causes errors: division by zero\n",
    "        if tp + fp == 0:\n",
    "            prec += 0\n",
    "        elif tp != 0:\n",
    "            prec += tp / (tp + fp)\n",
    "        else:\n",
    "            prec += 0\n",
    "            \n",
    "        #found out later that recall also causes errors\n",
    "        rec += tp / (tp + fn) if tp != 0 else 0\n",
    "    \n",
    "    acc = acc / num_classes\n",
    "    prec = prec / num_classes\n",
    "    rec = rec / num_classes\n",
    "    \n",
    "    f1 = 2 * prec * rec / (prec + rec) if prec + rec != 0 else 0\n",
    "    \n",
    "    ret = {\n",
    "        'accuracy':acc,\n",
    "        'precision':prec,\n",
    "        'recall':rec,\n",
    "        'f1-score':f1\n",
    "    } \n",
    "    return ret\n",
    "    \n",
    "def neural_network(dataset, architecture, num_folds, batch_size, num_iterations):\n",
    "    \"\"\"\n",
    "    Runs a neural network based on a dataset, architecture\n",
    "    - Cross validation is used for training with num_folds\n",
    "    \"\"\"\n",
    "    x,y = prepare_data(dataset, num_folds)\n",
    "    metrics_sum = []\n",
    "\n",
    "    for curr_fold in range(num_folds):\n",
    "        train_x, train_y, test_x, test_y = create_sets(\n",
    "            x, y, curr_fold)\n",
    "        \n",
    "        weights_list = initialize_weights(architecture, gradient=False)\n",
    "        empty_gradient = initialize_weights(architecture, gradient=True)\n",
    "        \n",
    "        gradients = copy.deepcopy(empty_gradient)\n",
    "        \n",
    "        cost_list = np.array([])\n",
    "        cost = 0\n",
    "        \n",
    "        instances_seen = 0\n",
    "        \n",
    "        #only for the final model\n",
    "        if FINAL_MODEL:\n",
    "            test_costs = []\n",
    "        \n",
    "        #stopping criteria:\n",
    "            #loops num_iterations times over the dataset\n",
    "        for _ in range(num_iterations):\n",
    "            \n",
    "            #loops over every iteration of dataset, updates weights when i is a multiple of batch_size\n",
    "            for i in range(train_x.shape[0]): \n",
    "                x_i = train_x[i].reshape(-1,1)\n",
    "                y_i = train_y[i].reshape(-1,1)\n",
    "                \n",
    "                #forward propagation\n",
    "                y_hat, activation_layers = forward_propagation(x_i, weights_list)\n",
    "                \n",
    "                #adds cost for this training instance\n",
    "                cost += compute_cost(y_hat, y_i)\n",
    "\n",
    "                #backward pass\n",
    "                deltas = compute_delta(weights_list.copy(), activation_layers.copy(), y_i)\n",
    "                \n",
    "                gradients = compute_gradients(activation_layers, deltas, gradients)\n",
    "                \n",
    "                #updates for the following conditional\n",
    "                instances_seen += 1\n",
    "                \n",
    "                #updates weights based on regularized gradients\n",
    "                if (instances_seen % batch_size) == 0:\n",
    "                    #regularizes cost and records:\n",
    "                    cost = cost / batch_size\n",
    "                    cost += regularize_cost(weights_list, batch_size)\n",
    "                    cost_list = np.append(cost_list, cost)\n",
    "                    #resets cost\n",
    "                    cost = 0\n",
    "                    \n",
    "                    #regularizes gradients\n",
    "                    reg_gradients = regularize_gradients(weights_list, gradients, batch_size)\n",
    "                    gradients = copy.deepcopy(empty_gradient)\n",
    "                \n",
    "                    #updates weights\n",
    "                    weights_list = update_weights(weights_list, reg_gradients)\n",
    "                    \n",
    "                    #observes and records cost for test data - same as below\n",
    "                    if FINAL_MODEL:\n",
    "                        test_cost = 0\n",
    "                        #first column is class, second is y_hat, third is true\n",
    "                        output = np.c_[np.arange(0, test_y.shape[1]), np.zeros(test_y.shape[1]), np.zeros(test_y.shape[1])]\n",
    "                        \n",
    "                        #forward propagation of test set\n",
    "                        for i in range(test_x.shape[0]):\n",
    "                            x_i = test_x[i].reshape(-1,1)\n",
    "                            y_i = test_y[i].reshape(-1,1)\n",
    "                            \n",
    "                            y_hat, activation_layers = forward_propagation(x_i, weights_list)\n",
    "                            test_cost += compute_cost(y_hat, y_i)\n",
    "                            \n",
    "                            y_hat_index = np.argmax(y_hat)\n",
    "                            y_true_index = np.argmax(y_i)\n",
    "                            output[y_hat_index, 1] += 1\n",
    "                            output[y_true_index, 2] += 1 \n",
    "                            \n",
    "                        test_cost += regularize_cost(weights_list, test_x.shape[0])\n",
    "                        test_costs.append(test_cost)\n",
    "                    \n",
    "        \n",
    "        #POST-TRAINING evaluation: \n",
    "        test_cost = 0\n",
    "        \n",
    "        #first column is class, second is y_hat, third is true\n",
    "        output = np.c_[np.arange(0, test_y.shape[1]), np.zeros(test_y.shape[1]), np.zeros(test_y.shape[1])]\n",
    "        \n",
    "        #forward propagation of test set\n",
    "        for i in range(test_x.shape[0]):\n",
    "            x_i = test_x[i].reshape(-1,1)\n",
    "            y_i = test_y[i].reshape(-1,1)\n",
    "            \n",
    "            y_hat, activation_layers = forward_propagation(x_i, weights_list)\n",
    "            test_cost += compute_cost(y_hat, y_i)\n",
    "            \n",
    "            y_hat_index = np.argmax(y_hat)\n",
    "            y_true_index = np.argmax(y_i)\n",
    "            output[y_hat_index, 1] += 1\n",
    "            output[y_true_index, 2] += 1 \n",
    "            \n",
    "        test_cost += regularize_cost(weights_list, test_x.shape[0])\n",
    "\n",
    "        #evaluates and records metrics\n",
    "        scores = record_confusion(output)\n",
    "\n",
    "        #metrics for one fold\n",
    "        metrics = {\n",
    "            'training-costs': cost_list,\n",
    "            'test-cost-final':test_cost,\n",
    "            'instances-seen':(instances_seen / num_folds)\n",
    "        }\n",
    "        metrics.update(scores)\n",
    "        \n",
    "        if FINAL_MODEL:\n",
    "            temp = {\n",
    "                'test-costs': test_costs\n",
    "            }\n",
    "            metrics.update(temp)\n",
    "            \n",
    "        metrics_sum.append(metrics)\n",
    "        \n",
    "    #compiles metrics and model hyperparameters\n",
    "    test = metrics_sum[0]\n",
    "    stats = {\n",
    "        'accuracy': [m['accuracy'] for m in metrics_sum],\n",
    "        'precision': [m['precision'] for m in metrics_sum],\n",
    "        'recall': [m['recall'] for m in metrics_sum],\n",
    "        'f1-score': [m['f1-score'] for m in metrics_sum],\n",
    "        'training-costs': [m['training-costs'] for m in metrics_sum],\n",
    "        'test-cost-final': [m['test-cost-final'] for m in metrics_sum],\n",
    "        'instances-seen': np.min([m['instances-seen'] for m in metrics_sum]),\n",
    "        'batch-size':batch_size,\n",
    "        'num-iterations': num_iterations,\n",
    "        'alpha': ALPHA,\n",
    "        'lambda': LAMBDA,\n",
    "        'architecture': architecture\n",
    "    }\n",
    "    if FINAL_MODEL:\n",
    "        temp = {\n",
    "            'test-costs': [m['test-costs'] for m in metrics_sum]\n",
    "        }\n",
    "        stats.update(temp)\n",
    "\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates graphs/records for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_test_cost(stats, batch_size, title):\n",
    "    fig, ax = plt.subplots()\n",
    "    costs = stats['test-costs']\n",
    "    \n",
    "    num_obs = stats['instances-seen']\n",
    "    \n",
    "    for i in range(len(costs)):\n",
    "        costs[i] = np.array(costs[i])\n",
    "        costs[i].resize(int(num_obs), refcheck=False)\n",
    "    \n",
    "    \n",
    "    costs = np.stack(costs, axis=0)\n",
    "    y = np.mean(costs, axis=0)\n",
    "\n",
    "    # mean_costs = np.mean(costs, axis=0)\n",
    "    \n",
    "    x = np.arange(0, int(num_obs))\n",
    "    \n",
    "    plt.plot(x,y, color='orange')\n",
    "    \n",
    "    plt.xlabel('Number of training iterations')\n",
    "    plt.ylabel('(Regularized) Training Cost')\n",
    "    \n",
    "    p_title = 'Training Cost for ' + title\n",
    "    plt.title(p_title)\n",
    "    \n",
    "    file_name = '../figures/' + title + '-test-cost.png'\n",
    "    plt.savefig(file_name)\n",
    "    \n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "\n",
    "def vis_convergence(stats, num_obs=-1):\n",
    "    \"\"\"\n",
    "    Helps visualize the cost function\n",
    "    \"\"\"\n",
    "    if num_obs == -1:\n",
    "        num_obs = stats['instances-seen']\n",
    "    \n",
    "    costs = stats['training-costs']\n",
    "    \n",
    "    for i in range(len(costs)):\n",
    "        costs[i] = np.array(costs[i])\n",
    "        costs[i].resize(int(num_obs), refcheck=False)\n",
    "    \n",
    "    # costs = stats['training-costs']\n",
    "    \n",
    "    costs = np.stack(costs, axis=0)\n",
    "    y = np.mean(costs, axis=0)\n",
    "    x = np.arange(0, y.shape[0])\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('')\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    ax.plot(x,y)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "#same thing as above but for tuning hyperparameters\n",
    "def vis_convergence_tune(stats, num_obs=-1):\n",
    "    \"\"\"\n",
    "    Helps visualize the cost function\n",
    "    \"\"\"\n",
    "    if num_obs == -1:\n",
    "        num_obs = stats['instances-seen']\n",
    "    \n",
    "    costs = stats['training-costs']\n",
    "    \n",
    "    for i in range(len(costs)):\n",
    "        costs[i] = np.array(costs[i])\n",
    "        costs[i].resize(int(num_obs), refcheck=False)\n",
    "    \n",
    "    # costs = stats['training-costs']\n",
    "    \n",
    "    costs = np.stack(costs, axis=0)\n",
    "    y = np.mean(costs, axis=0)\n",
    "    x = np.arange(0, y.shape[0])\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    ax.plot(x,y)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def compile_convergence_plots(convergence_plots, title):\n",
    "    fig, axes = plt.subplots(1, 6, figsize=(18, 6))\n",
    "    fig.subplots_adjust(bottom=0.2, wspace=0)\n",
    "    \n",
    "    for i in range(6):\n",
    "        axes[i].imshow(convergence_plots[i].get_figure().canvas.renderer.buffer_rgba())        \n",
    "        axes[i].set_xlabel(f'{i+1}')\n",
    "        axes[i].set_xticks([])\n",
    "        axes[i].set_yticks([])\n",
    "    \n",
    "    file_name = '../figures/' + title + '_converge.png'\n",
    "    \n",
    "    plt.savefig(file_name, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "def compress_metrics(stats, log=False):\n",
    "    acc = np.mean(stats['accuracy'])\n",
    "    rec = np.mean(stats['recall'])\n",
    "    prec = np.mean(stats['precision'])\n",
    "    f1 = np.mean(stats['f1-score'])\n",
    "    \n",
    "    mean_test_cost = np.mean(stats['test-cost-final'])\n",
    "    \n",
    "    if log:\n",
    "        print(f'accuracy: {acc:.4f}')\n",
    "        print(f'recall: {rec:.4f}')\n",
    "        print(f'precision: {prec:.4f}')\n",
    "        print(f'F1-Score: {f1:.4f}')\n",
    "        print(f'mean final test cost: {mean_test_cost:.4f}')\n",
    "    \n",
    "    ret = {\n",
    "        'm_accuracy':acc,\n",
    "        'm_recall':rec,\n",
    "        'm_precision':prec,\n",
    "        'm_f1-score':f1,\n",
    "        'm_test-cost-final':mean_test_cost\n",
    "    }\n",
    "    stats.update(ret)\n",
    "    return\n",
    "\n",
    "def accuracy_spread(model_data, title=''):\n",
    "    accuracies = []\n",
    "    for mod in model_data:\n",
    "        #accuracy box plot\n",
    "        accuracies.append(mod['accuracy'])\n",
    "    \n",
    "    fig, axes = plt.subplots(nrows=1, ncols=6, figsize=(6, 3), sharey=True)\n",
    "    \n",
    "    for i, ax in enumerate(axes):\n",
    "        ax.boxplot(accuracies[i])\n",
    "        ax.set_xlabel(f'Model {i+1}')\n",
    "        \n",
    "        ax.set_xticklabels([])\n",
    "        ax.tick_params(axis='both', which='both', length=0)\n",
    "\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.spines['bottom'].set_visible(False)\n",
    "        ax.spines['left'].set_visible(False)\n",
    "        \n",
    "        ax.grid(True)\n",
    "    \n",
    "    axes[0].set_ylabel('Accuracy Spread')\n",
    "        \n",
    "    plt.suptitle('Accuracy Spread')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    file_name = '../figures/' + title + '_acc_spread.png'\n",
    "    \n",
    "    plt.savefig(file_name)\n",
    "    plt.show()\n",
    "\n",
    "def summarize_to_latex(model_data):    \n",
    "    data = {}\n",
    "    \n",
    "    for i, m in zip(range(6), model_data):\n",
    "        one = round(m['alpha'], 3)\n",
    "        two = round(m['lambda'], 4)\n",
    "        three = m['architecture']\n",
    "        four = round(m['m_accuracy'], 4)\n",
    "        five = round(m['m_f1-score'], 4)\n",
    "        six = round(m['m_test-cost-final'],2)\n",
    "        seven = 0\n",
    "        \n",
    "        data[f'Model {i+1}'] = [one, two, three, four, five, six, seven]\n",
    "    \n",
    "    #seven columns\n",
    "    metrics = ['Learning Rate $\\\\alpha$', 'Regularization $\\\\lambda$', 'Architecture', 'Mean Accuracy',\n",
    "               'Mean F1-score', 'Mean Test Cost', 'Converges Around']\n",
    "    \n",
    "    df = pd.DataFrame(data, index=metrics)\n",
    "\n",
    "    latex_table = df.to_latex(float_format=\"%.4f\")\n",
    "\n",
    "    print(latex_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each dataset, I visually confirm that the training cost flatlines\n",
    "- I train each model individually and by hand to try and achieve the highest results, recording what I've done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wine dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wine():\n",
    "    global ALPHA, LAMBDA\n",
    "    \n",
    "    wine = pd.read_csv('datasets/wine.csv', sep='\\s+')\n",
    "    \n",
    "    #models:\n",
    "    #arch 4     a = 0.1, l = 0.1\n",
    "    #arch 4,4   a = 0.5, l = .02 \n",
    "    #arch 2     a = 0.5, l = 0.01\n",
    "    #arch 1     a = 0.7, l = 0.01\n",
    "    #arch 1,1,1 a = 0.1, l = 0.03\n",
    "    #arch 64,32 a = 0.5, l = 0.01\n",
    "\n",
    "    arch_list = [\n",
    "        [13,4,3],\n",
    "        [13,4,4,3],\n",
    "        [13,2,3],\n",
    "        [13,1,3],\n",
    "        [13,32,3],\n",
    "        [13,64,3]\n",
    "    ]\n",
    "    alpha_list = [0.1, 0.5, 0.5, 0.7, 0.1, 0.5]\n",
    "    lambda_list = [0.1, 0.02, 0.01, 0.01, 0.03, 0.01]\n",
    "\n",
    "    model_data = []\n",
    "    convergence_plots = []\n",
    "        \n",
    "    for i in range(6):\n",
    "        ALPHA = alpha_list[i]\n",
    "        LAMBDA = lambda_list[i]\n",
    "        model = neural_network(wine, arch_list[i], num_folds=10, batch_size=10, num_iterations=100)\n",
    "        compress_metrics(model, log=False)\n",
    "        \n",
    "        convergence_plots.append(vis_convergence(model))\n",
    "        \n",
    "        model_data.append(model)\n",
    "\n",
    "    #for the report\n",
    "    accuracy_spread(model_data, title='wine')\n",
    "    summarize_to_latex(model_data)\n",
    "    compile_convergence_plots(convergence_plots, title='wine')\n",
    "    \n",
    "    reset_hyperparameters()\n",
    "\n",
    "def wine_final():\n",
    "    global FINAL_MODEL, LAMBDA, ALPHA\n",
    "    FINAL_MODEL = True\n",
    "\n",
    "    #can't find the global for some reason\n",
    "    wine = pd.read_csv('datasets/wine.csv', sep='\\s+')\n",
    "\n",
    "    ALPHA = 0.5\n",
    "    LAMBDA = 0.01\n",
    "    model = neural_network(wine, [13,64,3], num_folds=10, batch_size=10, num_iterations=100)\n",
    "    compress_metrics(model, log=False)\n",
    "    plot_test_cost(model, 10, 'wine')\n",
    "\n",
    "    FINAL_MODEL = False\n",
    "    reset_hyperparameters()\n",
    "    return\n",
    "\n",
    "# wine()\n",
    "# wine_final()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def house():\n",
    "    global LAMBDA, ALPHA\n",
    "    house = pd.read_csv('datasets/house_votes_84.csv')\n",
    "    #models:\n",
    "    #arch 8     a = 0.2, l = 0.25\n",
    "    #arch 2,2   a = 0.5, l = 0\n",
    "    #arch 16,8  a = 0.3, l = 0.02\n",
    "    #arch 1,8   a = 0.5, l = 0.01\n",
    "    #arch 4,4,4,4  a = 0.25, l = 0.01   #very big jump l=0.01 to 0.02\n",
    "    #arch 32,32 a = 0.5, l = 0.05\n",
    "\n",
    "    arch_list = [\n",
    "        [16,8,2],\n",
    "        [16,2,2,2],\n",
    "        [16,16,8,2],\n",
    "        [16,1,8,2],\n",
    "        [16,4,4,4,4,2],\n",
    "        [16,32,32,2]\n",
    "    ]\n",
    "    alpha_list = [0.2, 0.5, 0.3, 0.5, 0.25, 0.5]\n",
    "    lambda_list = [0.25, 0, 0.02, 0.01, 0.01, 0.05]\n",
    "\n",
    "    model_data = []\n",
    "    convergence_plots = []\n",
    "        \n",
    "    for i in range(6):\n",
    "        ALPHA = alpha_list[i]\n",
    "        LAMBDA = lambda_list[i]\n",
    "        model = neural_network(house, arch_list[i], num_folds=10, batch_size=10, num_iterations=100)\n",
    "        compress_metrics(model, log=False)\n",
    "        \n",
    "        convergence_plots.append(vis_convergence(model))\n",
    "        \n",
    "        model_data.append(model)\n",
    "\n",
    "    #for the report\n",
    "    accuracy_spread(model_data, title='house')\n",
    "    summarize_to_latex(model_data)\n",
    "    compile_convergence_plots(convergence_plots, title='house')\n",
    "    \n",
    "    reset_hyperparameters()\n",
    "    return\n",
    "    \n",
    "def house_final():\n",
    "    global FINAL_MODEL, LAMBDA, ALPHA\n",
    "    FINAL_MODEL = True\n",
    "\n",
    "    #can't find the global for some reason\n",
    "    house = pd.read_csv('datasets/house_votes_84.csv')\n",
    "\n",
    "    ALPHA = 0.5\n",
    "    LAMBDA = 0\n",
    "    model = neural_network(house, [16,2,2,2], num_folds=10, batch_size=10, num_iterations=100)\n",
    "    compress_metrics(model, log=False)\n",
    "    plot_test_cost(model, 10, 'house')\n",
    "\n",
    "    FINAL_MODEL = False\n",
    "    reset_hyperparameters()\n",
    "    return\n",
    "\n",
    "# house()\n",
    "# house_final()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cancer Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cancer():\n",
    "    global LAMBDA, ALPHA\n",
    "\n",
    "#models:\n",
    "#arch 8,4   a = 0.2, l = 0.02\n",
    "#arch 2     a = 0.5, l = 0\n",
    "#arch 4,4   a = 0.3, l = 0.001\n",
    "#arch 32,32 a = 0.2, l = 0.05\n",
    "#arch 128  a = 0.25, l = 0      #very big jump l=0.01 to 0.02\n",
    "#arch 1, 32 a = 0.5, l = 0.05\n",
    "    arch_list = [\n",
    "        [9,8,4,2],\n",
    "        [9,2,2],\n",
    "        [9,4,4,2],\n",
    "        [9,32,32,2],\n",
    "        [9,128,2],\n",
    "        [9,1,32,2]\n",
    "    ]\n",
    "    alpha_list = [0.2, 0.5, 0.3, 0.2, 0.25, 0.5]\n",
    "    lambda_list = [0.02, 0, 0.001, 0.05, 0, 0.05]\n",
    "\n",
    "    model_data = []\n",
    "    convergence_plots = []\n",
    "        \n",
    "    for i in range(6):\n",
    "        ALPHA = alpha_list[i]\n",
    "        LAMBDA = lambda_list[i]\n",
    "        model = neural_network(cancer, arch_list[i], num_folds=10, batch_size=10, num_iterations=100)\n",
    "        compress_metrics(model, log=False)\n",
    "        \n",
    "        convergence_plots.append(vis_convergence(model))\n",
    "        \n",
    "        model_data.append(model)\n",
    "\n",
    "    #for the report\n",
    "    accuracy_spread(model_data, title='cancer')\n",
    "    summarize_to_latex(model_data)\n",
    "    compile_convergence_plots(convergence_plots, title='cancer')\n",
    "    \n",
    "    reset_hyperparameters()\n",
    "\n",
    "def cancer_final():\n",
    "    global FINAL_MODEL, LAMBDA, ALPHA\n",
    "    FINAL_MODEL = True\n",
    "\n",
    "    #can't find the global for some reason\n",
    "    cancer = pd.read_csv('datasets/cancer.csv', sep='\\s+')\n",
    "\n",
    "    #renames class column so my network can find output:\n",
    "    cancer.rename(columns={'Class': 'class'}, inplace=True)\n",
    "\n",
    "    ALPHA = 0.2\n",
    "    LAMBDA = 0.02\n",
    "    model = neural_network(cancer, [9,8,4,2], num_folds=10, batch_size=10, num_iterations=100)\n",
    "    compress_metrics(model, log=False)\n",
    "    plot_test_cost(model, 10, 'cancer')\n",
    "\n",
    "    FINAL_MODEL = False\n",
    "    reset_hyperparameters()\n",
    "    return\n",
    "\n",
    "# cancer()\n",
    "# cancer_final()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contraceptive-Use Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contraceptive():\n",
    "    global LAMBDA, ALPHA\n",
    "\n",
    "    #models:\n",
    "    #arch 8,8   a = 0.025, l = 0.001\n",
    "    #arch 32,32 a = 0.015, l = 0\n",
    "    #arch 2     a = 0.03, l = 0.001\n",
    "    #arch 64    a = 0.0005, l = 0.01\n",
    "    #arch 64,8  a = 0.005, l = 0.01      #very big jump l=0.01 to 0.02\n",
    "    #arch 8,4,4 a = 0.001, l = 0.2\n",
    "    arch_list = [\n",
    "        [9,8,8,3],\n",
    "        [9,32,32,3],\n",
    "        [9,2,3],\n",
    "        [9,64,3],\n",
    "        [9,64,8,3],\n",
    "        [9,8,4,4,3]\n",
    "    ]\n",
    "    alpha_list = [0.2, 0.5, 0.3, 0.2, 0.25, 0.5]\n",
    "    lambda_list = [0.02, 0, 0.001, 0.05, 0, 0.05]\n",
    "\n",
    "    model_data = []\n",
    "    convergence_plots = []\n",
    "        \n",
    "    for i in range(6):\n",
    "        ALPHA = alpha_list[i]\n",
    "        LAMBDA = lambda_list[i]\n",
    "        model = neural_network(contraceptive, arch_list[i], num_folds=10, batch_size=10, num_iterations=100)\n",
    "        compress_metrics(model, log=False)\n",
    "        \n",
    "        convergence_plots.append(vis_convergence(model))\n",
    "        \n",
    "        model_data.append(model)\n",
    "\n",
    "    #for the report\n",
    "    accuracy_spread(model_data, title='contraceptive')\n",
    "    summarize_to_latex(model_data)\n",
    "    compile_convergence_plots(convergence_plots, title='contraceptive')\n",
    "\n",
    "    reset_hyperparameters()\n",
    "\n",
    "def contraceptive_final():\n",
    "    global FINAL_MODEL, LAMBDA, ALPHA\n",
    "    FINAL_MODEL = True\n",
    "\n",
    "    #can't find the global for some reason\n",
    "    contraceptive = pd.read_csv('datasets/contraceptive+method+choice/cmc.data', header=None)\n",
    "\n",
    "    #renames class column so my network can find output:\n",
    "    contraceptive.rename(columns={9: 'class'}, inplace=True)\n",
    "\n",
    "    ALPHA = 0.2\n",
    "    LAMBDA = 0.02\n",
    "    model = neural_network(contraceptive, [9,8,8,3], num_folds=10, batch_size=10, num_iterations=100)\n",
    "    compress_metrics(model, log=False)\n",
    "    plot_test_cost(model, 10, 'Contraceptive Method Choice')\n",
    "\n",
    "    FINAL_MODEL = False\n",
    "    reset_hyperparameters()\n",
    "    return\n",
    "\n",
    "# contraceptive()\n",
    "# contraceptive_final()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP                                                   RESULT                             \n",
      "forward propagation instance 0\n",
      "\n",
      "       expected                                        0.79403\n",
      "       received                                        0.79403   \n",
      "\n",
      "forward propagation instance 1\n",
      "\n",
      "       expected                                        0.79597\n",
      "       received                                        0.79597   \n",
      "\n",
      "cost for instance 0\n",
      "\n",
      "       expected                                        0.366\n",
      "       received                                        0.36557\n",
      "cost for instance 1\n",
      "\n",
      "       expected                                        1.276\n",
      "       received                                        1.27638\n",
      "cumulative cost for step\n",
      "\n",
      "       expected                                        0.82098\n",
      "       received                                        0.82098\n",
      "\n",
      "backpass instance 1 -----------------\n",
      "expected delta layer3: [-0.10597], received: [[-0.10597257]]\n",
      "expected delta layer2: [-0.01270   -0.01548], \n",
      " received[[-0.01269739]\n",
      " [-0.01548092]]\n",
      "\n",
      "expected gradient theta 2: -0.10597  -0.06378  -0.06155  \n",
      "received [[-0.10597257 -0.06377504 -0.06154737]]\n",
      "\n",
      "expected gradient theta 1:\n",
      " -0.01270  -0.00165 \n",
      " -0.01548  -0.00201  \n",
      "received [[-0.01269739 -0.00165066]\n",
      " [-0.01548092 -0.00201252]]\n",
      "\n",
      "backpass instance 2 -----------------\n",
      "expected delta layer3: [0.56597], received: [[0.56596607]]\n",
      "expected delta layer2: [0.06740   0.08184], \n",
      " received[[0.06739994]\n",
      " [0.08184068]]\n",
      "\n",
      "expected gradient theta 2: 0.56597  0.34452  0.33666  \n",
      "received [[0.56596607 0.34452363 0.33665784]]\n",
      "\n",
      "expected gradient theta 1:\n",
      " 0.06740  0.02831 \n",
      " 0.08184  0.03437  \n",
      "received [[0.06739994 0.02830797]\n",
      " [0.08184068 0.03437309]]\n",
      "\n",
      "cummulative gradients -----------------\n",
      "expected gradient theta 2: 0.23000  0.14037  0.13756  \n",
      "received [[0.22999675 0.1403743  0.13755523]]\n",
      "\n",
      "expected gradient theta 1:\n",
      " 0.02735  0.01333 \n",
      " 0.03318  0.01618  \n",
      "received [[0.02735127 0.01332866]\n",
      " [0.03317988 0.01618028]]\n"
     ]
    }
   ],
   "source": [
    "def array_to_string(array, rank):\n",
    "    # if type(array) == np.float64:\n",
    "    #     return f'{array:.5f}'\n",
    "    \n",
    "    out = ''\n",
    "    if rank == 1:\n",
    "        for num in array:\n",
    "            out += f'{num[0]:.5f}' + '   '\n",
    "    elif rank == 2:\n",
    "        for arr in array:\n",
    "            for num in arr:\n",
    "                out += f'{num:.5f}' + '   '\n",
    "            out += '\\n'\n",
    "    return out\n",
    "\n",
    "def log_out(title, expected, received, tail=False):\n",
    "    if isinstance(received, np.float64):\n",
    "        received = f'{received:.5f}'\n",
    "    else:\n",
    "        received = array_to_string(received, len(received.shape))\n",
    "        \n",
    "    if tail:\n",
    "        tail = '\\n'\n",
    "    else:\n",
    "        tail = ''\n",
    "\n",
    "    print(title + '\\n\\n' + 'expected'.rjust(15).ljust(55) + expected + '\\n' +\n",
    "            'received'.rjust(15).ljust(55) + tail + received)\n",
    "    \n",
    "def run_ex1():\n",
    "    x = [0.13, 0.42]\n",
    "    y = [0.90, 0.23]\n",
    "    \n",
    "    # network_architecture = [1,2,1]\n",
    "    \n",
    "    weights1 = [\n",
    "        [0.4, 0.1],\n",
    "        [0.3, 0.2]\n",
    "    ]\n",
    "    weights2 = [\n",
    "        [0.7, 0.5, 0.6]\n",
    "    ]\n",
    "    \n",
    "    weights_list = [np.array(weights1), np.array(weights2)]\n",
    "    \n",
    "    gradients = [np.zeros((2,2)), np.zeros((1,3))]\n",
    "    \n",
    "    print('STEP'.ljust(55) + 'RESULT'.ljust(35))\n",
    "    \n",
    "    y_hat_1, act_layers1 = forward_propagation(x[0], weights_list)\n",
    "    \n",
    "    log_out('forward propagation instance 0', '0.79403', y_hat_1.reshape(1,-1))\n",
    "    \n",
    "    #example logs\n",
    "    # y_hat_1, actlist1 = forward_propagation(x[0], weights_list)\n",
    "    # log_out('forward propagation instance 0', '0.83318   0.84132', y_hat_1.reshape(1,-1))\n",
    "\n",
    "    # cost_1 = compute_cost(y_hat_1, y[0])\n",
    "    # log_out('cost for instance 0', '0.791', cost_1)\n",
    "    \n",
    "    # y_hat_2, actlist2 = forward_propagation(x[1], weights_list)\n",
    "    # log_out('forward propagation instance 1', '0.82953   0.83832', y_hat_2.reshape(1,-1))\n",
    "    \n",
    "    # cost_2 = compute_cost(y_hat_2, y[1])\n",
    "    # log_out('cost for instance 1', '1.944', cost_2)\n",
    "    \n",
    "    # cost = (cost_1 + cost_2) / 2\n",
    "    # cost += regularize_cost(weights_list, 2)\n",
    "    # log_out('cumulative cost for step', '1.90351', cost)\n",
    "    \n",
    "    y_hat_2, act_layers2 = forward_propagation(x[1], weights_list)\n",
    "    # print(f'instance 1, my_result: {y_hat_2[0]}, true: {.79597}')\n",
    "    \n",
    "    log_out('forward propagation instance 1', '0.79597', y_hat_2.reshape(1,-1))\n",
    "    \n",
    "    cost_1 = compute_cost(y_hat_1, y[0])\n",
    "    # print(f'expected cost 1, 0.366, received: {cost_1}')\n",
    "    \n",
    "    log_out('cost for instance 0', '0.366', cost_1)\n",
    "    \n",
    "    cost_2 = compute_cost(y_hat_2, y[1])\n",
    "    # print(f'expected cost 2, 1.276, received: {cost_2}')\n",
    "    \n",
    "    log_out('cost for instance 1', '1.276', cost_2)\n",
    "    \n",
    "    sum_cost = (cost_1 + cost_2) / 2\n",
    "    # print(f'expected J: 0.82098, received: {sum_cost}')\n",
    "    \n",
    "    log_out('cumulative cost for step', '0.82098', sum_cost)\n",
    "    \n",
    "    #backward pass\n",
    "    print('\\nbackpass instance 1 -----------------')\n",
    "    \n",
    "    deltas = compute_delta(weights_list.copy(), act_layers1.copy(), np.array(y[0]))\n",
    "    print(f'expected delta layer3: [-0.10597], received: {deltas[1]}')\n",
    "    print(f'expected delta layer2: [-0.01270   -0.01548], \\n received{deltas[0]}\\n')\n",
    "    \n",
    "    gradients1 = compute_gradients(act_layers1, deltas, copy.deepcopy(gradients))\n",
    "    print(f'expected gradient theta 2: -0.10597  -0.06378  -0.06155  \\nreceived {gradients1[1]}')\n",
    "    \n",
    "    print(f'\\nexpected gradient theta 1:\\n -0.01270  -0.00165 \\n -0.01548  -0.00201  \\nreceived {gradients1[0]}')\n",
    "    \n",
    "    \n",
    "    print('\\nbackpass instance 2 -----------------')\n",
    "    \n",
    "    deltas = compute_delta(weights_list.copy(), act_layers2.copy(), np.array(y[1]))\n",
    "    print(f'expected delta layer3: [0.56597], received: {deltas[1]}')\n",
    "    print(f'expected delta layer2: [0.06740   0.08184], \\n received{deltas[0]}\\n')\n",
    "    \n",
    "    gradients2= compute_gradients(act_layers2, deltas, copy.deepcopy(gradients))\n",
    "    print(f'expected gradient theta 2: 0.56597  0.34452  0.33666  \\nreceived {gradients2[1]}')\n",
    "    \n",
    "    print(f'\\nexpected gradient theta 1:\\n 0.06740  0.02831 \\n 0.08184  0.03437  \\nreceived {gradients2[0]}')\n",
    "    \n",
    "    \n",
    "    print('\\ncummulative gradients -----------------')\n",
    "    deltas = compute_delta(weights_list.copy(), act_layers2.copy(), np.array(y[1]))\n",
    "    gradients = compute_gradients(act_layers2, deltas, gradients1)\n",
    "    \n",
    "    for i in range(len(gradients)):\n",
    "        gradients[i] = gradients[i]/2\n",
    "    \n",
    "    print(f'expected gradient theta 2: 0.23000  0.14037  0.13756  \\nreceived {gradients[1]}')\n",
    "    \n",
    "    print(f'\\nexpected gradient theta 1:\\n 0.02735  0.01333 \\n 0.03318  0.01618  \\nreceived {gradients[0]}')\n",
    "   \n",
    "# my numbers match for example 1 \n",
    "run_ex1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP                                                   RESULT                             \n",
      "forward propagation instance 0\n",
      "\n",
      "       expected                                        0.83318   0.84132\n",
      "       received                                        0.83318   0.84132   \n",
      "\n",
      "cost for instance 0\n",
      "\n",
      "       expected                                        0.791\n",
      "       received                                        0.79074\n",
      "forward propagation instance 1\n",
      "\n",
      "       expected                                        0.82953   0.83832\n",
      "       received                                        0.82953   0.83832   \n",
      "\n",
      "cost for instance 1\n",
      "\n",
      "       expected                                        1.944\n",
      "       received                                        1.94378\n",
      "cumulative cost for step\n",
      "\n",
      "       expected                                        1.90351\n",
      "       received                                        1.90351\n",
      "Back Pass instance 0------------------\n",
      "\n",
      "delta layer 4\n",
      "\n",
      "       expected                                        0.08318   -0.13868\n",
      "       received                                        0.08318   -0.13868   \n",
      "\n",
      "delta layer 3\n",
      "\n",
      "       expected                                        0.00639   -0.00925   -0.00779\n",
      "       received                                        0.00639   -0.00925   -0.00779   \n",
      "\n",
      "delta layer 2\n",
      "\n",
      "       expected                                        -0.00087   -0.00133   -0.00053   -0.00070\n",
      "       received                                        -0.00087   -0.00133   -0.00053   -0.00070   \n",
      "\n",
      "gradient theta 3\n",
      "\n",
      "       expected                                        \n",
      "0.08318  0.07280  0.07427  0.06777\n",
      "-0.13868  -0.12138  -0.12384  -0.11300\n",
      "       received                                        \n",
      "0.08318   0.07280   0.07427   0.06777   \n",
      "-0.13868   -0.12138   -0.12384   -0.11300   \n",
      "\n",
      "gradient theta 2\n",
      "\n",
      "       expected                                        \n",
      "0.00639  0.00433  0.00482  0.00376  0.00451\n",
      "-0.00925  -0.00626  -0.00698  -0.00544  -0.00653\n",
      "-0.00779  -0.00527  -0.00587  -0.00458  -0.00550\n",
      "       received                                        \n",
      "0.00639   0.00433   0.00482   0.00376   0.00451   \n",
      "-0.00925   -0.00626   -0.00698   -0.00544   -0.00653   \n",
      "-0.00779   -0.00527   -0.00587   -0.00458   -0.00550   \n",
      "\n",
      "gradient theta 1\n",
      "\n",
      "       expected                                        \n",
      "-0.00087  -0.00028  -0.00059\n",
      "-0.00133  -0.00043  -0.00091\n",
      "-0.00053  -0.00017  -0.00036\n",
      "-0.00070  -0.00022  -0.00048\n",
      "       received                                        \n",
      "-0.00087   -0.00028   -0.00059   \n",
      "-0.00133   -0.00043   -0.00091   \n",
      "-0.00053   -0.00017   -0.00036   \n",
      "-0.00070   -0.00022   -0.00048   \n",
      "\n",
      "Back Pass instance 1------------------\n",
      "\n",
      "delta layer 4\n",
      "\n",
      "       expected                                        0.07953   0.55832\n",
      "       received                                        0.07953   0.55832   \n",
      "\n",
      "delta layer 3\n",
      "\n",
      "       expected                                        0.01503   0.05809   0.06892\n",
      "       received                                        0.01503   0.05809   0.06892   \n",
      "\n",
      "delta layer 2\n",
      "\n",
      "       expected                                        0.01694   0.01465   0.01999   0.01622\n",
      "       received                                        0.01694   0.01465   0.01999   0.01622   \n",
      "\n",
      "gradient theta 3\n",
      "\n",
      "       expected                                        \n",
      "0.07953  0.06841  0.07025  0.06346\n",
      "0.55832  0.48027  0.49320  0.44549\n",
      "       received                                        \n",
      "0.07953   0.06841   0.07025   0.06346   \n",
      "0.55832   0.48027   0.49320   0.44549   \n",
      "\n",
      "gradient theta 2\n",
      "\n",
      "       expected                                        \n",
      "0.01503  0.00954  0.01042  0.00818  0.00972\n",
      "0.05809  0.03687  0.04025  0.03160  0.03756\n",
      "0.06892  0.04374  0.04775  0.03748  0.04456\n",
      "\n",
      "       received                                        \n",
      "0.01503   0.00954   0.01042   0.00818   0.00972   \n",
      "0.05809   0.03687   0.04025   0.03160   0.03756   \n",
      "0.06892   0.04374   0.04775   0.03748   0.04456   \n",
      "\n",
      "gradient theta 1\n",
      "\n",
      "       expected                                        \n",
      "0.01694  0.01406  0.00034\n",
      "0.01465  0.01216  0.00029\n",
      "0.01999  0.01659  0.00040\n",
      "0.01622  0.01346  0.00032\n",
      "       received                                        \n",
      "0.01694   0.01406   0.00034   \n",
      "0.01465   0.01216   0.00029   \n",
      "0.01999   0.01659   0.00040   \n",
      "0.01622   0.01346   0.00032   \n",
      "\n",
      "Regularized Gradients------------------\n",
      "\n",
      "reg gradient theta 3\n",
      "\n",
      "       expected                                        \n",
      "0.08135  0.17935  0.12476  0.13186\n",
      "0.20982  0.19195  0.30343  0.25249\n",
      "       received                                        \n",
      "0.08135   0.17935   0.12476   0.13186   \n",
      "0.20982   0.19195   0.30343   0.25249   \n",
      "\n",
      "reg gradient theta 2\n",
      "\n",
      "       expected                                        \n",
      "0.01071  0.09068  0.02512  0.12597  0.11586\n",
      "0.02442  0.06780  0.04164  0.05308  0.12677\n",
      "0.03056  0.08924  0.12094  0.10270  0.03078 \n",
      "       received                                        \n",
      "0.01071   0.09068   0.02512   0.12597   0.11586   \n",
      "0.02442   0.06780   0.04164   0.05308   0.12677   \n",
      "0.03056   0.08924   0.12094   0.10270   0.03078   \n",
      "\n",
      "reg gradient theta 1\n",
      "\n",
      "       expected                                        \n",
      "0.00804  0.02564  0.04987\n",
      "0.00666  0.01837  0.06719\n",
      "0.00973  0.03196  0.05252\n",
      "0.00776  0.05037  0.08492\n",
      "       received                                        \n",
      "0.00804   0.02564   0.04987   \n",
      "0.00666   0.01837   0.06719   \n",
      "0.00973   0.03196   0.05252   \n",
      "0.00776   0.05037   0.08492   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def run_ex2():\n",
    "    global LAMBDA\n",
    "    LAMBDA = 0.250\n",
    "    \n",
    "    weights1 = [\n",
    "        [0.42000, 0.15000, 0.40000],\n",
    "        [0.72000, 0.10000, 0.54000],\n",
    "        [0.01000, 0.19000, 0.42000],\n",
    "        [0.30000, 0.35000, 0.68000]\n",
    "    ]\n",
    "    weights2 = [\n",
    "        [0.21000, 0.67000, 0.14000, 0.96000, 0.87000],\n",
    "        [0.87000, 0.42000, 0.20000, 0.32000, 0.89000],\n",
    "        [0.03000, 0.56000, 0.80000, 0.69000, 0.09000]\n",
    "    ]\n",
    "    weights3 = [\n",
    "        [0.04000, 0.87000, 0.42000, 0.53000],\n",
    "        [0.17000, 0.10000, 0.95000, 0.69000]\n",
    "    ]\n",
    "    weights_list = [np.array(weights1),np.array(weights2),np.array(weights3)]\n",
    "    \n",
    "    empty_gradients = initialize_weights(dimensions=[2,4,3,2], gradient=True)\n",
    "    \n",
    "    x = [np.array([0.32000, 0.68000]), np.array([0.83000, 0.02000])]\n",
    "    y = [np.array([0.75000, 0.98000]), np.array([0.75000, 0.28000])]\n",
    "    \n",
    "    x = [x_i.reshape(-1,1) for x_i in x]\n",
    "    y = [y_i.reshape(-1,1) for y_i in y]\n",
    "    \n",
    "    print('STEP'.ljust(55) + 'RESULT'.ljust(35))\n",
    "    \n",
    "    y_hat_1, actlist1 = forward_propagation(x[0], weights_list)\n",
    "    log_out('forward propagation instance 0', '0.83318   0.84132', y_hat_1.reshape(1,-1))\n",
    "\n",
    "    cost_1 = compute_cost(y_hat_1, y[0])\n",
    "    log_out('cost for instance 0', '0.791', cost_1)\n",
    "    \n",
    "    y_hat_2, actlist2 = forward_propagation(x[1], weights_list)\n",
    "    log_out('forward propagation instance 1', '0.82953   0.83832', y_hat_2.reshape(1,-1))\n",
    "    \n",
    "    cost_2 = compute_cost(y_hat_2, y[1])\n",
    "    log_out('cost for instance 1', '1.944', cost_2)\n",
    "    \n",
    "    cost = (cost_1 + cost_2) / 2\n",
    "    cost += regularize_cost(weights_list, 2)\n",
    "    log_out('cumulative cost for step', '1.90351', cost)\n",
    "    \n",
    "    print('Back Pass instance 0------------------\\n')\n",
    "    deltas = compute_delta(copy.deepcopy(weights_list), copy.deepcopy(actlist1), y[0])\n",
    "    log_out('delta layer 4', '0.08318   -0.13868', deltas[2].reshape(1,-1))\n",
    "    log_out('delta layer 3', '0.00639   -0.00925   -0.00779', deltas[1].reshape(1,-1))\n",
    "    log_out('delta layer 2', '-0.00087   -0.00133   -0.00053   -0.00070', deltas[0].reshape(1,-1))\n",
    "    \n",
    "    gradients = compute_gradients(actlist1, deltas, copy.deepcopy(empty_gradients))\n",
    "    log_out('gradient theta 3', '\\n0.08318  0.07280  0.07427  0.06777\\n-0.13868  -0.12138  -0.12384  -0.11300', gradients[2], True)\n",
    "    log_out('gradient theta 2', '\\n0.00639  0.00433  0.00482  0.00376  0.00451\\n-0.00925  -0.00626  -0.00698  -0.00544  -0.00653\\n-0.00779  -0.00527  -0.00587  -0.00458  -0.00550', gradients[1], True)\n",
    "    log_out('gradient theta 1', '\\n-0.00087  -0.00028  -0.00059\\n-0.00133  -0.00043  -0.00091\\n-0.00053  -0.00017  -0.00036\\n-0.00070  -0.00022  -0.00048', gradients[0], True)\n",
    "    \n",
    "    print('Back Pass instance 1------------------\\n')\n",
    "    deltas = compute_delta(copy.deepcopy(weights_list), copy.deepcopy(actlist2), y[1])\n",
    "    log_out('delta layer 4', '0.07953   0.55832', copy.deepcopy(deltas[2]).reshape(1,-1))\n",
    "    log_out('delta layer 3', '0.01503   0.05809   0.06892', copy.deepcopy(deltas[1]).reshape(1,-1))\n",
    "    log_out('delta layer 2', '0.01694   0.01465   0.01999   0.01622', copy.deepcopy(deltas[0]).reshape(1,-1))\n",
    "    \n",
    "    #somethings going wrong in the [1:] obs of each gradient TODO\n",
    "    gradients2 = compute_gradients(actlist2, deltas, copy.deepcopy(empty_gradients))\n",
    "    log_out('gradient theta 3', '\\n0.07953  0.06841  0.07025  0.06346\\n0.55832  0.48027  0.49320  0.44549', gradients2[2], True)\n",
    "    log_out('gradient theta 2', '\\n0.01503  0.00954  0.01042  0.00818  0.00972\\n0.05809  0.03687  0.04025  0.03160  0.03756\\n0.06892  0.04374  0.04775  0.03748  0.04456\\n', gradients2[1], True)\n",
    "    log_out('gradient theta 1', '\\n0.01694  0.01406  0.00034\\n0.01465  0.01216  0.00029\\n0.01999  0.01659  0.00040\\n0.01622  0.01346  0.00032', gradients2[0], True)\n",
    "    \n",
    "    print('Regularized Gradients------------------\\n')\n",
    "    gradients = compute_gradients(actlist2, deltas, gradients)\n",
    "\n",
    "    regularized_gradients = regularize_gradients(weights_list, gradients, 2)\n",
    "    log_out('reg gradient theta 3', '\\n0.08135  0.17935  0.12476  0.13186\\n0.20982  0.19195  0.30343  0.25249', regularized_gradients[2], True)\n",
    "    log_out('reg gradient theta 2', '\\n0.01071  0.09068  0.02512  0.12597  0.11586\\n0.02442  0.06780  0.04164  0.05308  0.12677\\n0.03056  0.08924  0.12094  0.10270  0.03078 ', regularized_gradients[1], True)\n",
    "    log_out('reg gradient theta 1', '\\n0.00804  0.02564  0.04987\\n0.00666  0.01837  0.06719\\n0.00973  0.03196  0.05252\\n0.00776  0.05037  0.08492', regularized_gradients[0], True)\n",
    "    \n",
    "# my numbers match for example 2\n",
    "run_ex2()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
