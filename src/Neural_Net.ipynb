{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "#digits from sci-kit learn\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading datasets. Digits requires some additional pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parkinsons dataset\n",
    "parkinsons = pd.read_csv('./datasets/parkinsons.csv')\n",
    "parkinsons.rename(columns={'Diagnosis': 'class'}, inplace=True)\n",
    "\n",
    "#titanic dataset\n",
    "titanic = pd.read_csv('./datasets/titanic_processed.csv')\n",
    "\n",
    "#digits from sci-kit learn\n",
    "def load_digits():\n",
    "    dig = datasets.load_digits(return_X_y=True)\n",
    "\n",
    "    X_df = pd.DataFrame(dig[0])\n",
    "    y_df = pd.DataFrame(dig[1], columns=['class'])\n",
    "    \n",
    "    dig = pd.concat([X_df, y_df], axis=1)\n",
    "    return dig\n",
    "\n",
    "digits = load_digits()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Globals are used as hyper-parameters in very specific places. Each dataset/example test should reset these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL_MODEL = False\n",
    "def reset_hyperparameters():\n",
    "    global LAMBDA, ALPHA\n",
    "    LAMBDA = 0\n",
    "    ALPHA = 1\n",
    "    \n",
    "reset_hyperparameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing data\n",
    "- cross-validation with stratified sampling\n",
    "- normalization of data\n",
    "- one-hot encoding of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_training_data(dataset):\n",
    "    \"\"\"\n",
    "    Normalizes non-class columns to the [0,1] range\n",
    "    returns non-class columns\n",
    "    \"\"\"\n",
    "    for j in range(dataset.shape[1]):\n",
    "        max = np.max(dataset[:,j])\n",
    "        min = np.min(dataset[:,j])\n",
    "        \n",
    "        if (min - max != 0):\n",
    "            dataset[:,j] = (dataset[:,j] - min)/(max-min)\n",
    "        else:\n",
    "            dataset[:,j] = 0\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def vectorize_classes(class_labels):\n",
    "    \"\"\"\n",
    "    one-hot encoding of class labels\n",
    "    - returns: list of vectors representing y\n",
    "    \"\"\"\n",
    "    classes = np.unique(class_labels)\n",
    "    classes = np.sort(classes)\n",
    "    \n",
    "    #first index is index of second class in codified data\n",
    "    num_classes = len(classes)\n",
    "    index_col = np.arange(0, num_classes)\n",
    "    classes = np.c_[index_col, classes]\n",
    "    \n",
    "    y = []\n",
    "    \n",
    "    for i in range(class_labels.shape[0]):\n",
    "        curr_class = class_labels[i]\n",
    "        class_index = np.where(classes[:, 1] == curr_class)[0][0]\n",
    "        \n",
    "        # one hot encoding\n",
    "        \n",
    "        #reshapes into vector\n",
    "        y_i = np.zeros((num_classes), dtype=np.int64)\n",
    "        \n",
    "        y_i[class_index] = 1\n",
    "        y.append(y_i)\n",
    "    \n",
    "    #converts to n by num_classes matrix\n",
    "    y = np.vstack(y)\n",
    "\n",
    "    return y\n",
    "\n",
    "def create_folds(x, y, num_folds):\n",
    "    \"\"\"\n",
    "    creates arrays of sub datasets used for cross-validation\n",
    "    - x and y have to be treated differently\n",
    "    \"\"\"\n",
    "    x_folds = []\n",
    "    y_folds = []\n",
    "    \n",
    "    #roughly the number of observations per class\n",
    "    fold_size = x.shape[0] / num_folds\n",
    "    \n",
    "    #folds have correct distribution of classes\n",
    "    for i in range(num_folds):\n",
    "        #prevents uneven distribution of fold-sizes\n",
    "        lower = i * fold_size\n",
    "        if (i < num_folds - 1):\n",
    "            upper = (i + 1) * fold_size\n",
    "        else:\n",
    "            upper = x.shape[0]\n",
    "        \n",
    "        x_fold = x[int(lower):int(upper)]\n",
    "        y_fold = y[int(lower):int(upper)]\n",
    "        \n",
    "        x_folds.append(x_fold)\n",
    "        y_folds.append(y_fold)\n",
    "        \n",
    "    return x_folds, y_folds\n",
    "    \n",
    "def prepare_data(dataset, num_folds):\n",
    "    #shuffling data - this is the only time we do so\n",
    "    dataset = dataset.sample(frac=1)\n",
    "    \n",
    "    #separates class_labels\n",
    "    class_labels = dataset.pop('class')\n",
    "    \n",
    "    #converts to numpy matrix\n",
    "    x = dataset.to_numpy()\n",
    "    class_labels = class_labels.to_numpy()\n",
    "    \n",
    "    #normalizes x data to [0,1] range\n",
    "    #this prevents overflow errors and helps processing\n",
    "    x = normalize_training_data(x)\n",
    "    \n",
    "    #encodes classes into vectors (one hot encoding)\n",
    "    y = vectorize_classes(class_labels)\n",
    "    \n",
    "    #creates an array of sub-datasets used later for cross-validation\n",
    "    x_folds, y_folds = create_folds(x, y, num_folds)\n",
    "\n",
    "    return x_folds, y_folds\n",
    "    \n",
    "def create_sets(x, y, curr_fold):\n",
    "    \"\"\"\n",
    "    Returns train and test sets for cross-validation\n",
    "    - Converts x data frames into numpy matrix\n",
    "    \"\"\"\n",
    "    x = copy.deepcopy(x)\n",
    "    y = copy.deepcopy(y)\n",
    "    \n",
    "    #test data, we pop one fold\n",
    "    test_x = x.pop(curr_fold)\n",
    "    \n",
    "    test_y = y.pop(curr_fold)\n",
    "    \n",
    "    #train data\n",
    "    train_x = np.vstack(x)\n",
    "    train_y = np.vstack(y)\n",
    "    \n",
    "    return train_x, train_y, test_x, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(dimensions, gradient:bool):\n",
    "    \"\"\"\n",
    "    Initializes weights\n",
    "    \"\"\"\n",
    "    matrices = []\n",
    "    for i in range(len(dimensions) - 1):\n",
    "\n",
    "        if gradient:\n",
    "            weights = np.zeros((dimensions[i+1], dimensions[i]+1))\n",
    "        else:\n",
    "            #samples initial theta matrices from a standard normal distribution\n",
    "            weights = np.random.randn(dimensions[i+1], dimensions[i]+1)\n",
    "            \n",
    "        matrices.append(weights)\n",
    "    return matrices\n",
    "\n",
    "def forward_propagation(instance, weights_list):\n",
    "    \"\"\"\n",
    "    Forward propagates a training instance \n",
    "        returns final layer of network\n",
    "                and all layers of neural network\n",
    "    \"\"\"\n",
    "    activated_layers = []\n",
    "    \n",
    "    #activates input layer\n",
    "    # act_layer = sigmoid(instance)\n",
    "    act_layer = np.array(instance)\n",
    "    \n",
    "    #reshapes into a vector\n",
    "    act_layer = act_layer.reshape(-1,1)\n",
    "    \n",
    "    #adds a bias term\n",
    "    act_layer = np.insert(act_layer, 0, values=1)\n",
    "    \n",
    "    #reshapes again after insertion\n",
    "    act_layer = act_layer.reshape(-1,1)\n",
    "    \n",
    "    #appends\n",
    "    activated_layers.append(act_layer)\n",
    "    \n",
    "    for weights in weights_list:\n",
    "        layer = np.dot(weights, act_layer)\n",
    "        \n",
    "        #sigmoid function layer\n",
    "        act_layer = sigmoid(layer)\n",
    "        \n",
    "        #adding bias for next phase\n",
    "        act_layer = np.insert(act_layer, 0, values=1)   \n",
    "        \n",
    "        #reshapes after insertion\n",
    "        act_layer = act_layer.reshape(-1,1)\n",
    "        \n",
    "        #store the activated layer for back-propagation\n",
    "        activated_layers.append(act_layer)\n",
    "        \n",
    "\n",
    "    y_hat = act_layer[1:]\n",
    "    activated_layers[len(activated_layers)-1] = activated_layers[len(activated_layers)-1][1:]\n",
    "    \n",
    "    return y_hat, activated_layers\n",
    "\n",
    "def sigmoid(vector):\n",
    "    return 1 / (1 + np.exp(-vector))\n",
    "\n",
    "def compute_cost(y_hat, y_true):\n",
    "    #dimensions must match\n",
    "    cost_arr = - y_true * np.log(y_hat) - (1-y_true) * np.log(1-y_hat)\n",
    "    #common overflow error\n",
    "    return np.sum(cost_arr)\n",
    "\n",
    "def regularize_cost(weights_list, batch_size):\n",
    "    \"\"\"\n",
    "    Adds cost for overfitting\n",
    "        Computes squared sum of all non-bias weights\n",
    "    \"\"\"\n",
    "    sum = 0\n",
    "    for weights in weights_list:\n",
    "        weights = weights[:, 1:]\n",
    "        weights = np.square(weights)\n",
    "        sum += np.sum(weights)\n",
    "    \n",
    "    sum = LAMBDA * sum / (2 * batch_size) \n",
    "    return sum\n",
    "\n",
    "def compute_delta(weights_list, activation_layers, y_true):\n",
    "    \"\"\"\n",
    "    Computes delta vectors for each hidden layer\n",
    "    \"\"\"\n",
    "    #we remove from the end and exclude the first layer\n",
    "    #we also pop to exclude the final pred. layer\n",
    "    \n",
    "    #delta of prediction layer\n",
    "    delta_next = activation_layers.pop() \n",
    "    delta_next = delta_next - y_true.reshape(-1,1)\n",
    "    \n",
    "    #stores deltas in the same order as activation layers\n",
    "    deltas = [delta_next]\n",
    "    \n",
    "    while len(activation_layers) > 1:\n",
    "        thetas = weights_list.pop()\n",
    "        activation = activation_layers.pop()\n",
    "        activation_2 = 1 - activation\n",
    "        \n",
    "        delta_curr = np.dot(np.transpose(thetas), delta_next)\n",
    "        delta_curr = delta_curr * activation * activation_2\n",
    "        \n",
    "        #removes delta associated with bias node\n",
    "        delta_curr = delta_curr[1:]\n",
    "        \n",
    "        #reshapes into a vector\n",
    "        # delta_curr = delta_curr.reshape(-1,1)\n",
    "        \n",
    "        #stores and resets next delta layer\n",
    "        deltas.insert(0,delta_curr)\n",
    "        delta_next = delta_curr\n",
    "        \n",
    "    return deltas\n",
    "\n",
    "def compute_gradients(activation_layers, deltas, previous_gradients):\n",
    "    \"\"\"\n",
    "    - computes gradients based on activations of neurons and next layer deltas\n",
    "    - adds these to the previous gradients\n",
    "    \"\"\"\n",
    "    #deltas list starts at first hidden layer (second layer), \n",
    "    for i in range(len(activation_layers) - 1):\n",
    "        activation = np.transpose(activation_layers[i])\n",
    "        delta_t = deltas[i]\n",
    "        \n",
    "        gradient_layer = np.dot(delta_t, activation)\n",
    "\n",
    "        #accumulates to previous gradients\n",
    "        previous_gradients[i] += gradient_layer\n",
    "        \n",
    "    return previous_gradients\n",
    "\n",
    "def regularize_gradients(weights_list, prev_gradients, batch_size):\n",
    "    \"\"\"\n",
    "    Computes a regularized gradient which punishes large weights\n",
    "    - adds it to the previous gradient and normalizes it for final updates\n",
    "    \"\"\"\n",
    "    # regularization if any\n",
    "    reg_gradients = []\n",
    "    for weights in weights_list:\n",
    "        reg_gradient = LAMBDA * copy.deepcopy(weights)\n",
    "        reg_gradient[:,0] = 0\n",
    "        reg_gradients.append(reg_gradient)\n",
    "    \n",
    "    for i in range(len(prev_gradients)):\n",
    "        reg_gradients[i] += prev_gradients[i]\n",
    "        \n",
    "    for i in range(len(reg_gradients)):\n",
    "        reg_gradients[i] = reg_gradients[i] / batch_size\n",
    "    \n",
    "    return reg_gradients\n",
    "\n",
    "def update_weights(weights_list, reg_gradients):\n",
    "    \"\"\"\n",
    "    Updates weights based on regularized gradients\n",
    "    \"\"\"\n",
    "    for i in range(len(weights_list)):\n",
    "        weights_list[i] = weights_list[i] - ALPHA * reg_gradients[i]\n",
    "    \n",
    "    return weights_list\n",
    "\n",
    "def record_confusion(output):\n",
    "    \"\"\"\n",
    "    Compiles accuracy, recall, precision, and f1 score based on output\n",
    "    \"\"\"\n",
    "    acc = 0\n",
    "    prec = 0\n",
    "    rec = 0\n",
    "    num_classes = output.shape[0]\n",
    "    n = np.sum(output[:,1])\n",
    "    \n",
    "    #for each class compute performance metrics\n",
    "    for c in range(num_classes):\n",
    "        tp, tn, fp, fn = 0,0,0,0\n",
    "        \n",
    "        pred_pos = output[c,1]\n",
    "        act_pos = output[c,2]\n",
    "        \n",
    "        pred_neg = np.sum(output[:,1]) - pred_pos\n",
    "        act_neg = np.sum(output[:,2]) - act_pos\n",
    "        \n",
    "        tp = pred_pos if pred_pos <= act_pos else act_pos\n",
    "        tn = pred_neg if pred_neg <= act_neg else act_neg\n",
    "        fn = act_pos - tp\n",
    "        fp = act_neg - tn\n",
    "        \n",
    "        acc += (tp + tn) / n\n",
    "        \n",
    "        #precision causes errors: division by zero\n",
    "        if tp + fp == 0:\n",
    "            prec += 0\n",
    "        elif tp != 0:\n",
    "            prec += tp / (tp + fp)\n",
    "        else:\n",
    "            prec += 0\n",
    "            \n",
    "        #found out later that recall also causes errors\n",
    "        rec += tp / (tp + fn) if tp != 0 else 0\n",
    "    \n",
    "    acc = acc / num_classes\n",
    "    prec = prec / num_classes\n",
    "    rec = rec / num_classes\n",
    "    \n",
    "    f1 = 2 * prec * rec / (prec + rec) if prec + rec != 0 else 0\n",
    "    \n",
    "    ret = {\n",
    "        'accuracy':acc,\n",
    "        'precision':prec,\n",
    "        'recall':rec,\n",
    "        'f1-score':f1\n",
    "    } \n",
    "    return ret\n",
    "    \n",
    "def neural_network(dataset, architecture, num_folds, batch_size, num_iterations):\n",
    "    \"\"\"\n",
    "    Runs a neural network based on a dataset, architecture\n",
    "    - Cross validation is used for training with num_folds\n",
    "    \"\"\"\n",
    "    x,y = prepare_data(dataset, num_folds)\n",
    "    metrics_sum = []\n",
    "\n",
    "    for curr_fold in range(num_folds):\n",
    "        train_x, train_y, test_x, test_y = create_sets(\n",
    "            x, y, curr_fold)\n",
    "        \n",
    "        weights_list = initialize_weights(architecture, gradient=False)\n",
    "        empty_gradient = initialize_weights(architecture, gradient=True)\n",
    "        \n",
    "        gradients = copy.deepcopy(empty_gradient)\n",
    "        \n",
    "        cost_list = np.array([])\n",
    "        cost = 0\n",
    "        \n",
    "        instances_seen = 0\n",
    "        \n",
    "        #only for the final model\n",
    "        if FINAL_MODEL:\n",
    "            test_costs = []\n",
    "        \n",
    "        #stopping criteria:\n",
    "            #loops num_iterations times over the dataset\n",
    "        for _ in range(num_iterations):\n",
    "            \n",
    "            #loops over every iteration of dataset, updates weights when i is a multiple of batch_size\n",
    "            for i in range(train_x.shape[0]): \n",
    "                x_i = train_x[i].reshape(-1,1)\n",
    "                y_i = train_y[i].reshape(-1,1)\n",
    "                \n",
    "                #forward propagation\n",
    "                y_hat, activation_layers = forward_propagation(x_i, weights_list)\n",
    "                \n",
    "                #adds cost for this training instance\n",
    "                cost += compute_cost(y_hat, y_i)\n",
    "\n",
    "                #backward pass\n",
    "                deltas = compute_delta(weights_list.copy(), activation_layers.copy(), y_i)\n",
    "                \n",
    "                gradients = compute_gradients(activation_layers, deltas, gradients)\n",
    "                \n",
    "                #updates for the following conditional\n",
    "                instances_seen += 1\n",
    "                \n",
    "                #updates weights based on regularized gradients\n",
    "                if (instances_seen % batch_size) == 0:\n",
    "                    #regularizes cost and records:\n",
    "                    cost = cost / batch_size\n",
    "                    cost += regularize_cost(weights_list, batch_size)\n",
    "                    cost_list = np.append(cost_list, cost)\n",
    "                    #resets cost\n",
    "                    cost = 0\n",
    "                    \n",
    "                    #regularizes gradients\n",
    "                    reg_gradients = regularize_gradients(weights_list, gradients, batch_size)\n",
    "                    gradients = copy.deepcopy(empty_gradient)\n",
    "                \n",
    "                    #updates weights\n",
    "                    weights_list = update_weights(weights_list, reg_gradients)\n",
    "                    \n",
    "                    #observes and records cost for test data - same as below\n",
    "                    if FINAL_MODEL:\n",
    "                        test_cost = 0\n",
    "                        #first column is class, second is y_hat, third is true\n",
    "                        output = np.c_[np.arange(0, test_y.shape[1]), np.zeros(test_y.shape[1]), np.zeros(test_y.shape[1])]\n",
    "                        \n",
    "                        #forward propagation of test set\n",
    "                        for i in range(test_x.shape[0]):\n",
    "                            x_i = test_x[i].reshape(-1,1)\n",
    "                            y_i = test_y[i].reshape(-1,1)\n",
    "                            \n",
    "                            y_hat, activation_layers = forward_propagation(x_i, weights_list)\n",
    "                            test_cost += compute_cost(y_hat, y_i)\n",
    "                            \n",
    "                            y_hat_index = np.argmax(y_hat)\n",
    "                            y_true_index = np.argmax(y_i)\n",
    "                            output[y_hat_index, 1] += 1\n",
    "                            output[y_true_index, 2] += 1 \n",
    "                            \n",
    "                        test_cost += regularize_cost(weights_list, test_x.shape[0])\n",
    "                        test_costs.append(test_cost)\n",
    "                    \n",
    "        \n",
    "        #POST-TRAINING evaluation: \n",
    "        test_cost = 0\n",
    "        \n",
    "        #first column is class, second is y_hat, third is true\n",
    "        output = np.c_[np.arange(0, test_y.shape[1]), np.zeros(test_y.shape[1]), np.zeros(test_y.shape[1])]\n",
    "        \n",
    "        #forward propagation of test set\n",
    "        for i in range(test_x.shape[0]):\n",
    "            x_i = test_x[i].reshape(-1,1)\n",
    "            y_i = test_y[i].reshape(-1,1)\n",
    "            \n",
    "            y_hat, activation_layers = forward_propagation(x_i, weights_list)\n",
    "            test_cost += compute_cost(y_hat, y_i)\n",
    "            \n",
    "            y_hat_index = np.argmax(y_hat)\n",
    "            y_true_index = np.argmax(y_i)\n",
    "            output[y_hat_index, 1] += 1\n",
    "            output[y_true_index, 2] += 1 \n",
    "            \n",
    "        test_cost += regularize_cost(weights_list, test_x.shape[0])\n",
    "\n",
    "        #evaluates and records metrics\n",
    "        scores = record_confusion(output)\n",
    "\n",
    "        #metrics for one fold\n",
    "        metrics = {\n",
    "            'training-costs': cost_list,\n",
    "            'test-cost-final':test_cost,\n",
    "            'instances-seen':(instances_seen / num_folds)\n",
    "        }\n",
    "        metrics.update(scores)\n",
    "        \n",
    "        if FINAL_MODEL:\n",
    "            temp = {\n",
    "                'test-costs': test_costs\n",
    "            }\n",
    "            metrics.update(temp)\n",
    "            \n",
    "        metrics_sum.append(metrics)\n",
    "        \n",
    "    #compiles metrics and model hyperparameters\n",
    "    test = metrics_sum[0]\n",
    "    stats = {\n",
    "        'accuracy': [m['accuracy'] for m in metrics_sum],\n",
    "        'precision': [m['precision'] for m in metrics_sum],\n",
    "        'recall': [m['recall'] for m in metrics_sum],\n",
    "        'f1-score': [m['f1-score'] for m in metrics_sum],\n",
    "        'training-costs': [m['training-costs'] for m in metrics_sum],\n",
    "        'test-cost-final': [m['test-cost-final'] for m in metrics_sum],\n",
    "        'instances-seen': np.min([m['instances-seen'] for m in metrics_sum]),\n",
    "        'batch-size':batch_size,\n",
    "        'num-iterations': num_iterations,\n",
    "        'alpha': ALPHA,\n",
    "        'lambda': LAMBDA,\n",
    "        'architecture': architecture\n",
    "    }\n",
    "    if FINAL_MODEL:\n",
    "        temp = {\n",
    "            'test-costs': [m['test-costs'] for m in metrics_sum]\n",
    "        }\n",
    "        stats.update(temp)\n",
    "\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates graphs/records for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_test_cost(stats, batch_size, title):\n",
    "    fig, ax = plt.subplots()\n",
    "    costs = stats['test-costs']\n",
    "    \n",
    "    num_obs = stats['instances-seen']\n",
    "    \n",
    "    for i in range(len(costs)):\n",
    "        costs[i] = np.array(costs[i])\n",
    "        costs[i].resize(int(num_obs), refcheck=False)\n",
    "    \n",
    "    \n",
    "    costs = np.stack(costs, axis=0)\n",
    "    y = np.mean(costs, axis=0)\n",
    "\n",
    "    # mean_costs = np.mean(costs, axis=0)\n",
    "    \n",
    "    x = np.arange(0, int(num_obs))\n",
    "    \n",
    "    plt.plot(x,y, color='orange')\n",
    "    \n",
    "    plt.xlabel('Number of training iterations')\n",
    "    plt.ylabel('(Regularized) Test Cost')\n",
    "    \n",
    "    p_title = 'Test Cost for ' + title\n",
    "    plt.title(p_title)\n",
    "    \n",
    "    file_name = './figures/' + title + '-test-cost.png'\n",
    "    plt.savefig(file_name)\n",
    "    \n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "def vis_convergence_tune(stats, num_obs=-1, title=''):\n",
    "    \"\"\"\n",
    "    Helps visualize the cost function\n",
    "    \"\"\"\n",
    "    if num_obs == -1:\n",
    "        num_obs = stats['instances-seen']\n",
    "    \n",
    "    costs = stats['training-costs']\n",
    "    \n",
    "    for i in range(len(costs)):\n",
    "        costs[i] = np.array(costs[i])\n",
    "        costs[i].resize(int(num_obs), refcheck=False)\n",
    "    \n",
    "    # costs = stats['training-costs']\n",
    "    \n",
    "    costs = np.stack(costs, axis=0)\n",
    "    y = np.mean(costs, axis=0)\n",
    "    x = np.arange(0, y.shape[0])\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    plt.title(f'Training Cost per Training Iterations Seen - {title} Dataset')\n",
    "    plt.xlabel('Number of Training Instances Seen')\n",
    "    plt.ylabel('Cost')\n",
    "    \n",
    "\n",
    "    plt.tight_layout()\n",
    "    ax.plot(x,y)\n",
    "    \n",
    "    plt.savefig(f'./figures/{title}_train_cost.png')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "def compress_metrics(stats, log=False):\n",
    "    acc = np.mean(stats['accuracy'])\n",
    "    rec = np.mean(stats['recall'])\n",
    "    prec = np.mean(stats['precision'])\n",
    "    f1 = np.mean(stats['f1-score'])\n",
    "    \n",
    "    mean_test_cost = np.mean(stats['test-cost-final'])\n",
    "    \n",
    "    if log:\n",
    "        print(f'accuracy: {acc:.4f}')\n",
    "        print(f'recall: {rec:.4f}')\n",
    "        print(f'precision: {prec:.4f}')\n",
    "        print(f'F1-Score: {f1:.4f}')\n",
    "        print(f'mean final test cost: {mean_test_cost:.4f}')\n",
    "    \n",
    "    ret = {\n",
    "        'm_accuracy':acc,\n",
    "        'm_recall':rec,\n",
    "        'm_precision':prec,\n",
    "        'm_f1-score':f1,\n",
    "        'm_test-cost-final':mean_test_cost\n",
    "    }\n",
    "    stats.update(ret)\n",
    "    return\n",
    "\n",
    "def summarize_to_latex(model_data):    \n",
    "    data = {}\n",
    "    \n",
    "    for i, m in zip(range(len(model_data)), model_data):\n",
    "        one = round(m['alpha'], 3)\n",
    "        two = round(m['lambda'], 4)\n",
    "        three = m['architecture']\n",
    "        four = round(m['m_accuracy'], 4)\n",
    "        five = round(m['m_f1-score'], 4)\n",
    "        six = round(m['m_test-cost-final'],2)\n",
    "        seven = 0\n",
    "        \n",
    "        data[f'Model {i+1}'] = [one, two, three, four, five, six, seven]\n",
    "    \n",
    "    #seven columns\n",
    "    metrics = ['Learning Rate $\\\\alpha$', 'Regularization $\\\\lambda$', 'Architecture', 'Mean Accuracy',\n",
    "               'Mean F1-score', 'Mean Test Cost', 'Converges Around']\n",
    "    \n",
    "    df = pd.DataFrame(data, index=metrics)\n",
    "\n",
    "    latex_table = df.to_latex(float_format=\"%.4f\")\n",
    "\n",
    "    print(latex_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each dataset, I visually confirm that the training cost flatlines\n",
    "- I train each model individually and by hand to try and achieve the highest results, recording what I've done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_digits():\n",
    "    global ALPHA, LAMBDA, FINAL_MODEL\n",
    "    \n",
    "    ALPHA = 0.05\n",
    "    LAMBDA = 0.01\n",
    "    FINAL_MODEL = True\n",
    "    model = neural_network(digits, architecture=[64, 32, 32, 10], num_folds=10, batch_size=10, num_iterations=100)\n",
    "\n",
    "    compress_metrics(model, log=True)\n",
    "    vis_convergence_tune(model, -1, title='Digits')\n",
    "\n",
    "    summarize_to_latex(model_data=[model])\n",
    "\n",
    "    plot_test_cost(model, batch_size=10, title='Digits')\n",
    "    \n",
    "# run_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_titanic():\n",
    "    global ALPHA, LAMBDA, FINAL_MODEL\n",
    "\n",
    "    ALPHA = 0.04\n",
    "    LAMBDA = 0.09    #0.03\n",
    "    FINAL_MODEL = True\n",
    "\n",
    "    model = neural_network(titanic, architecture=[6, 4, 2], num_folds=10, batch_size=10, num_iterations=100)\n",
    "\n",
    "    compress_metrics(model, log=True)\n",
    "    vis_convergence_tune(model, -1, title='Titanic')\n",
    "\n",
    "    summarize_to_latex(model_data=[model])\n",
    "    plot_test_cost(model, batch_size=10, title='Titanic')\n",
    "\n",
    "# run_titanic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_parkinsons():\n",
    "    global ALPHA, LAMBDA, FINAL_MODEL\n",
    "\n",
    "    ALPHA = 0.1\n",
    "    LAMBDA = 0.0001    #0.03\n",
    "    FINAL_MODEL = True\n",
    "\n",
    "    model = neural_network(parkinsons, architecture=[22, 12, 2], num_folds=10, batch_size=10, num_iterations=100)\n",
    "\n",
    "    compress_metrics(model, log=True)\n",
    "    vis_convergence_tune(model, -1, title='Parkinson\\'s')\n",
    "\n",
    "    summarize_to_latex(model_data=[model])\n",
    "    plot_test_cost(model, batch_size=10, title='Parikinson\\'s')\n",
    "\n",
    "# run_parkinsons()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
