{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries and Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# import warnings\n",
    "# with warnings.catch_warnings():\n",
    "#     warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "\"\"\"Default Global Values for splitting criteria in decision trees\"\"\"\n",
    "MINIMAL_SIZE_FOR_SPLIT = 1\n",
    "\n",
    "MAXIMUM_DEPTH = 1000\n",
    "\n",
    "MINIMAL_INFO_GAIN = 0\n",
    "\n",
    "optimize_numerical = False\n",
    "\n",
    "random.seed(1)\n",
    "TREE_NUMS = [1,2,5,10,20,30,50]\n",
    "NUM_FOLDS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree ADT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    #building the tree\n",
    "    def __init__(self, is_numeric):\n",
    "        self.isLeaf = False\n",
    "        self.numeric = is_numeric\n",
    "        \n",
    "    def set_attribute(self, attribute, split_value):\n",
    "        self.attribute = attribute\n",
    "        self.split_value = split_value\n",
    "        \n",
    "    def set_as_leaf(self, decision):\n",
    "        self.isLeaf = True\n",
    "        self.decision = decision\n",
    "        \n",
    "    def set_left(self, node):\n",
    "        self.left = node\n",
    "        \n",
    "    def set_right(self, node):\n",
    "        self.right = node\n",
    "        \n",
    "    #making a decision\n",
    "    def get_attribute(self):\n",
    "        return self.attribute\n",
    "    def get_split_value(self):\n",
    "        return self.split_value\n",
    "    def is_numeric(self):\n",
    "        return self.numeric\n",
    "    \n",
    "    def get_left(self):\n",
    "        return self.left\n",
    "    def get_right(self):\n",
    "        return self.right\n",
    "    \n",
    "    def is_leaf(self):\n",
    "        return self.isLeaf\n",
    "    def get_decision(self):\n",
    "        return self.decision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree mechanics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_entropy(dataset):\n",
    "    dataset = dataset['class']\n",
    "    N = dataset.shape[0]\n",
    "    class_list = dataset.value_counts()\n",
    "\n",
    "    entropy = 0\n",
    "    for count in class_list.values:\n",
    "        prob = count / N\n",
    "        log_prob = np.log2(prob)\n",
    "        entropy -= prob * log_prob\n",
    "    return entropy\n",
    "\n",
    "def split_dataset(dataset, numerical, attribute, split_val):\n",
    "    if numerical:\n",
    "        left = dataset[dataset[attribute] < split_val]\n",
    "        right = dataset[dataset[attribute] >= split_val]\n",
    "        return left, right\n",
    "    else:\n",
    "        right = dataset.loc[dataset[attribute] == split_val]\n",
    "        left = dataset.loc[dataset[attribute] != split_val]\n",
    "        return left, right\n",
    "\n",
    "def entropy_column(dataset, attribute):\n",
    "    att_name = attribute.iloc[0]\n",
    "    data_type = attribute.iloc[1]\n",
    "    N = dataset.shape[0]\n",
    "    \n",
    "    \n",
    "\n",
    "    if data_type == 'numerical':\n",
    "        if optimize_numerical:\n",
    "            split = np.mean(dataset.iloc[:,0])\n",
    "            \n",
    "            left, right = split_dataset(dataset, numerical=True, attribute=att_name, split_val=split)\n",
    "            left_length = left.shape[0]\n",
    "            right_length = right.shape[0]\n",
    "            \n",
    "            left_entropy = compute_entropy(left)\n",
    "            right_entropy = compute_entropy(right)\n",
    "\n",
    "            entropy = left_entropy * (left_length/N) + right_entropy * (right_length/N)\n",
    "            return entropy, split\n",
    "        \n",
    "        #this method sorts the column by ascending unique values, takes the average of two consecutive values,\n",
    "        #   and performs the split on the resulting value. Then the one with the lowest resulting entropy is chosen\n",
    "        #   and returns both the entropy and split val\n",
    "        dataset = dataset.sort_values(by=dataset.columns[0])\n",
    "        \n",
    "        #gets unique values and creates information tracker\n",
    "        att_values = dataset.iloc[:, 0].unique()\n",
    "        split_entropy = np.c_[np.zeros(len(att_values) - 1), np.zeros(len(att_values) - 1)]\n",
    "        \n",
    "        \n",
    "        for i in range(len(att_values)-1):\n",
    "            split = (att_values[i] + att_values[i+1])/2\n",
    "            split_entropy[i, 0] = split\n",
    "            \n",
    "            #splits dataset above and below split\n",
    "            left, right = split_dataset(dataset, numerical=True, attribute=att_name, split_val=split)\n",
    "            \n",
    "            left_length = left.shape[0]\n",
    "            right_length = right.shape[0]\n",
    "            \n",
    "            left_entropy = compute_entropy(left)\n",
    "            right_entropy = compute_entropy(right)\n",
    "\n",
    "            entropy = left_entropy * (left_length/N) + right_entropy * (right_length/N)\n",
    "            split_entropy[i,1] = entropy\n",
    "        \n",
    "        if split_entropy.shape[0] == 0:\n",
    "            return 0, None\n",
    "        if split_entropy.shape[0] == 1:\n",
    "            return split_entropy[0,1], split_entropy[0,0]\n",
    "        \n",
    "        min_entropy_index = np.argmin(split_entropy[:,1])\n",
    "        \n",
    "        #first item returned is the entropy, second is the value we choose to split at\n",
    "        return split_entropy[min_entropy_index,1], split_entropy[min_entropy_index, 0]\n",
    "    \n",
    "    else:\n",
    "        assert data_type == 'categorical'\n",
    "        \n",
    "        category_values = dataset[att_name].value_counts()\n",
    "        categories = np.transpose(category_values.axes)\n",
    "        \n",
    "        split_entropy = pd.DataFrame(data=np.c_[categories, np.zeros(categories.shape[0])], columns=['category', 'entropy'])\n",
    "        categories = categories.flatten()\n",
    "        \n",
    "        for cat in categories:\n",
    "            part_one, part_two = split_dataset(dataset, numerical=False, attribute=att_name, split_val=cat)\n",
    "            \n",
    "            part_one_entropy = compute_entropy(part_one)\n",
    "            part_two_entropy = compute_entropy(part_two)\n",
    "            \n",
    "            n = category_values[cat] #num times cat appears in dataset\n",
    "            entropy = part_one_entropy * n / N + part_two_entropy * (N-n) / N #weighed avg\n",
    "            split_entropy.loc[split_entropy['category'] == cat, 'entropy'] = entropy\n",
    "        \n",
    "        min_row = np.argmin(split_entropy['entropy'])\n",
    "        \n",
    "        return split_entropy.loc[min_row, 'entropy'], split_entropy.loc[min_row, 'category']\n",
    "\n",
    "def determine_attribute(dataset, attribute_list):\n",
    "    attribute_list = attribute_list.copy()\n",
    "    \n",
    "    presplit_entropy = compute_entropy(dataset)\n",
    "    \n",
    "    #sets up information for debugging and decision\n",
    "    #attribute_list has columns 'attribute', 'attribute type', 'information gain', and 'split_value\n",
    "    info_gain = np.ones(attribute_list.shape[0]) * presplit_entropy #stores info_gain\n",
    "    \n",
    "    #this column either contains the numerical split value or class to separate\n",
    "    split_value = np.zeros(attribute_list.shape[0])\n",
    "    \n",
    "    attribute_list.insert(2, 'info_gain', info_gain)\n",
    "    attribute_list.insert(3, 'split_value', split_value)\n",
    "    attribute_list = attribute_list.drop(attribute_list[attribute_list['attribute'] == 'class'].index)\n",
    "    attribute_list = attribute_list.reset_index(drop=True)\n",
    "    \n",
    "    #computes info gain for each column\n",
    "    for i in range(len(attribute_list)):\n",
    "        attribute = attribute_list.loc[i, 'attribute']\n",
    "        \n",
    "        column = dataset[[attribute, 'class']]\n",
    "        \n",
    "        gain, split_val = entropy_column(column, attribute_list.iloc[i])\n",
    "        \n",
    "        attribute_list.loc[i, 'info_gain'] -= gain\n",
    "        attribute_list.loc[i, 'split_value'] = split_val\n",
    "    \n",
    "    #sorts and chooses attribute with highest info gain\n",
    "    attribute_list = attribute_list.sort_values(by='info_gain', ascending=False)\n",
    "    return attribute_list.iloc[0]\n",
    "\n",
    "def get_majority_class(dataset):\n",
    "    dataset = dataset['class']\n",
    "    highest_class = dataset.value_counts().index[0]\n",
    "    return highest_class\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_tree(dataset, attribute_list, depth=0):\n",
    "    #stopping criteria:\n",
    "    curr_entropy = compute_entropy(dataset)\n",
    "    if (curr_entropy == 0 or attribute_list.shape[0] <= MINIMAL_SIZE_FOR_SPLIT or depth >= MAXIMUM_DEPTH): \n",
    "        leaf = Node(False)\n",
    "        decision = get_majority_class(dataset)\n",
    "        leaf.set_as_leaf(decision)\n",
    "        return leaf\n",
    "    \n",
    "    #determines best attribute to split on\n",
    "    split_attribute = determine_attribute(dataset, attribute_list)\n",
    "    \n",
    "    #more stopping criteria based on information gain\n",
    "    if split_attribute['info_gain'] <= MINIMAL_INFO_GAIN: \n",
    "        leaf = Node(False)\n",
    "        decision = get_majority_class(dataset)\n",
    "        leaf.set_as_leaf(decision)\n",
    "        return leaf\n",
    "    \n",
    "    #splits dataset based on the value of the attribute\n",
    "    #   for categorical: right node if attribute equals split value else left node\n",
    "    #   for numerical: right node if attribute greater than or equal to split value else left node\n",
    "    numerical = True if split_attribute['att_type'] == 'numerical' else False\n",
    "    \n",
    "    parent = Node(is_numeric=numerical)\n",
    "    parent.set_attribute(split_attribute['attribute'], split_attribute['split_value'])\n",
    "    \n",
    "    left_dataset, right_dataset = split_dataset(dataset, numerical, \n",
    "                                                split_attribute['attribute'], \n",
    "                                                split_val=split_attribute['split_value'])\n",
    "    \n",
    "    #removes category once we split on it\n",
    "    if not numerical:\n",
    "        attribute_name = split_attribute['attribute']\n",
    "        attribute_list = attribute_list[attribute_list['attribute'] != attribute_name]\n",
    "        \n",
    "        parent.set_attribute(split_attribute['attribute'], split_attribute['split_value'])\n",
    "    \n",
    "    \n",
    "    #recursively creates child nodes\n",
    "    if len(left_dataset) == 0: #set to majority class in dataset if split results in complete decision\n",
    "        left_node = Node(False)\n",
    "        decision = get_majority_class(dataset)\n",
    "        left_node.set_as_leaf(decision)\n",
    "    else:\n",
    "        if left_dataset.size == 0:\n",
    "            print('error')\n",
    "        left_node = decision_tree(left_dataset, attribute_list, depth=(depth+1))\n",
    "\n",
    "    if len(right_dataset) == 0:\n",
    "        right_node = Node(False)\n",
    "        decision = get_majority_class(dataset)\n",
    "        right_node.set_as_leaf(decision)\n",
    "    else:\n",
    "        right_node = decision_tree(right_dataset, attribute_list, depth=(depth+1))\n",
    "    \n",
    "    \n",
    "    parent.set_left(left_node)\n",
    "    parent.set_right(right_node)\n",
    "    \n",
    "    return parent\n",
    "\n",
    "def traverse_tree(tree, observation):\n",
    "    while not tree.is_leaf():\n",
    "        curr_att = tree.get_attribute()\n",
    "        is_numeric = tree.is_numeric()\n",
    "        split_val = tree.get_split_value()\n",
    "        x = observation[curr_att]\n",
    "        if is_numeric:\n",
    "            if x >= split_val:\n",
    "                tree = tree.get_right()\n",
    "            else:\n",
    "                tree = tree.get_left()\n",
    "        else:\n",
    "            if x == split_val:\n",
    "                tree = tree.get_right()\n",
    "            else:\n",
    "                tree = tree.get_left()\n",
    "    decision = tree.get_decision()\n",
    "    return decision\n",
    "\n",
    "def predict(tree, dataset):\n",
    "    #y_hat and y_true\n",
    "    y_hat = np.empty(dataset.shape[0], dtype='int')\n",
    "    \n",
    "    for i in range(dataset.shape[0]):\n",
    "        observation = dataset.iloc[i]\n",
    "        y_hat[i] = traverse_tree(tree, observation)\n",
    "    \n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing cross-validation, metrics, and bootstrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratify_dataset(dataset, folds):\n",
    "    classes = dataset['class'].value_counts()\n",
    "    class_info = pd.DataFrame({'class': classes.index, 'count': classes.values})\n",
    "    classes = list(classes.axes)\n",
    "    class_info['fold_size'] = np.floor(class_info['count'] / folds)\n",
    "    class_info['remainder'] = class_info['count'] % folds\n",
    "\n",
    "    dataset = dataset.sample(frac=1)\n",
    "    subsets = []\n",
    "    for fold in range(folds):\n",
    "        new_fold = pd.DataFrame(columns=dataset.columns)\n",
    "        for c in class_info['class']:\n",
    "            fold_size = int(class_info.loc[class_info['class'] == c, 'fold_size'].iloc[0])\n",
    "            if class_info.loc[class_info['class'] == c, 'remainder'].iloc[0] > 0:\n",
    "                class_info.loc[class_info['class'] == c, 'remainder'] -= 1\n",
    "                fold_size += 1\n",
    "            \n",
    "            observations = dataset.loc[dataset['class'] == c].head(fold_size)\n",
    "            new_fold = pd.concat([new_fold, observations], ignore_index=True)\n",
    "            # new_fold = new_fold.append(observations, ignore_index=True)\n",
    "            dataset = dataset.drop(observations.index)\n",
    "        new_fold = new_fold.sample(frac=1)\n",
    "        subsets.append(new_fold)\n",
    "    \n",
    "    return subsets\n",
    "\n",
    "def make_statistics(output):\n",
    "    n = output.shape[0]\n",
    "    \n",
    "    #makes df counting confusion matrix values:\n",
    "    classes = list(output['y_true'].value_counts().index)\n",
    "    num_classes = len(classes)\n",
    "    \n",
    "    true_pos = np.zeros(num_classes)\n",
    "    false_pos = np.zeros(num_classes)\n",
    "    true_neg = np.zeros(num_classes)\n",
    "    false_neg = np.zeros(num_classes)\n",
    "    \n",
    "    data = {\n",
    "    'class': classes,\n",
    "    'true_pos': true_pos,\n",
    "    'true_neg': true_neg,\n",
    "    'false_pos': false_pos,\n",
    "    'false_neg': false_neg\n",
    "    }\n",
    "    \n",
    "    confusion_matrix = pd.DataFrame(data, dtype='int')\n",
    "    \n",
    "    #classifies prediction by true and false pos and neg\n",
    "    for i in range(n):\n",
    "        y_true = output.loc[i, 'y_true']\n",
    "        y_hat = output.loc[i, 'y_hat_majority']\n",
    "        class_row = confusion_matrix.index[confusion_matrix['class'] == y_true][0]\n",
    "        \n",
    "        if y_hat == y_true:\n",
    "            #adds true positive for y_true\n",
    "            confusion_matrix.loc[class_row, 'true_pos'] += 1\n",
    "            \n",
    "            #adds true negatives for every class except y_true\n",
    "            for j in range(num_classes):\n",
    "                if j == class_row:\n",
    "                    continue\n",
    "                else:\n",
    "                    confusion_matrix.loc[j, 'true_neg'] += 1\n",
    "        else: #prediction is different from true class\n",
    "            \n",
    "            #adds a false negative for y_true\n",
    "            confusion_matrix.loc[class_row, 'false_neg'] += 1\n",
    "            \n",
    "            #adds false positives for every other class\n",
    "            for j in range(num_classes):\n",
    "                if j == class_row:\n",
    "                    continue\n",
    "                else:\n",
    "                    confusion_matrix.loc[j, 'false_pos'] += 1\n",
    "    \n",
    "    accuracy = confusion_matrix['true_pos'].sum() / n\n",
    "\n",
    "    precision = []\n",
    "    recall = []\n",
    "    for c in range(num_classes):\n",
    "        tp = confusion_matrix.loc[c, 'true_pos']\n",
    "        fp = confusion_matrix.loc[c, 'false_pos']\n",
    "        fn = confusion_matrix.loc[c, 'false_neg']\n",
    "        \n",
    "        precision_c = tp / (tp + fp)\n",
    "        recall_c = tp / (tp + fn)\n",
    "        \n",
    "        precision.append(precision_c)\n",
    "        recall.append(recall_c)\n",
    "        \n",
    "    precision = np.mean(precision)\n",
    "    recall = np.mean(recall)\n",
    "\n",
    "    beta = 1\n",
    "    f1_score = (1 + np.square(beta)) * (precision * recall) / (np.square(beta) * precision + recall)\n",
    "    \n",
    "    statistics = {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1-score': f1_score\n",
    "    }\n",
    "    return statistics\n",
    "\n",
    "def subset_attributes(attribute_list):\n",
    "    \n",
    "    #samples ~a^(1/2) atributes (with replacement) to train subtree\n",
    "    num_attributes = attribute_list.shape[0]\n",
    "    num_subcolumns = int(np.floor(np.sqrt(num_attributes)) + 1)\n",
    "    \n",
    "    attribute_sublist = np.empty(shape=(num_subcolumns,2), dtype='<U100')\n",
    "    \n",
    "    for i in range(num_subcolumns):\n",
    "        r = random.randint(0, (num_attributes - 1)) #inclusive\n",
    "        \n",
    "        att = attribute_list.loc[r,'attribute']\n",
    "        att_type = attribute_list.loc[r,'att_type']\n",
    "        \n",
    "        while att == 'class':\n",
    "            r = random.randint(0, (num_attributes - 1))\n",
    "            att = attribute_list.loc[r,'attribute']\n",
    "            att_type = attribute_list.loc[r,'att_type']\n",
    "        \n",
    "        attribute_sublist[i] = [att, att_type]\n",
    "    \n",
    "    attribute_sublist = pd.DataFrame(data=attribute_sublist, columns=attribute_list.columns)\n",
    "    \n",
    "    return attribute_sublist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compiles Decision trees into forrests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_forest(train, test, attribute_list, output, ntree):\n",
    "    #creates ntree voters which classify the test set\n",
    "    for t in range(ntree):\n",
    "        \n",
    "        #Bootstrap Aggregating:\n",
    "        \n",
    "        attribute_sublist = subset_attributes(attribute_list)\n",
    "        \n",
    "        tree = decision_tree(train, attribute_sublist)\n",
    "        \n",
    "        y_hat = predict(tree, test)\n",
    "        label = 'y_hat' + str(t)\n",
    "        output[label] = y_hat\n",
    "    \n",
    "    return output\n",
    "    \n",
    "    \n",
    "def random_forest(dataset, attribute_list, num_folds, ntree):\n",
    "    dataset = dataset.sample(frac=1) #shuffles dataset\n",
    "    \n",
    "    subsets = stratify_dataset(dataset, num_folds)\n",
    "\n",
    "    #prepares output for statistics\n",
    "    columns = ['y_true', 'y_hat_majority']\n",
    "    output_total = pd.DataFrame(columns=columns)\n",
    "    \n",
    "    #k-folding done here:\n",
    "    for k in range(num_folds):\n",
    "        subsets_copy = subsets.copy()\n",
    "        \n",
    "        test = subsets_copy.pop(k)\n",
    "\n",
    "        y_true = test['class']\n",
    "        output = pd.DataFrame(data=y_true.values,columns=['y_true'])\n",
    "        \n",
    "        train = pd.concat(subsets_copy, ignore_index=True)\n",
    "        \n",
    "        output = run_forest(train,test, attribute_list, output, ntree)\n",
    "        \n",
    "        majority_vote = np.zeros(output.shape[0])\n",
    "        if ntree > 1:\n",
    "            majority_vote = [output.iloc[row, 1:ntree].mode().iloc[0] for row in range(output.shape[0])]\n",
    "        else:\n",
    "            majority_vote = output['y_hat0']\n",
    "        \n",
    "        output = pd.DataFrame(data={'y_true': output['y_true'], 'y_hat_majority': majority_vote})\n",
    "        output_total = pd.concat([output_total, output], ignore_index=True)\n",
    "    \n",
    "    statistics = make_statistics(output_total)\n",
    "    return statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizing the stop condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_hyperparameters():\n",
    "    MAXIMUM_DEPTH = 10000\n",
    "    MINIMAL_SIZE_FOR_SPLIT = 1\n",
    "    MINIMAL_SIZE_FOR_SPLIT = 1\n",
    "    return\n",
    "    \n",
    "def optimize_stop_condition(dataset, attributes, ntree=5, print_out=False):\n",
    "    #sorry for this code, i got really lazy here because of a bug somewhere else\n",
    "    \n",
    "    #MINIMAL_SIZE_FOR_SPLIT, MAXIMUM_DEPTH, MINIMAL_INFO_GAIN\n",
    "    #default: 0, really big, 0\n",
    "    reset_hyperparameters()\n",
    "\n",
    "    #1:20:2     DEPTH\n",
    "    depth = np.c_[np.zeros(10),np.zeros(10)]\n",
    "    ind=0\n",
    "    for i in range(3,22,2):\n",
    "        MAXIMUM_DEPTH = i\n",
    "        out = random_forest(dataset, attributes, num_folds=NUM_FOLDS, ntree=ntree)\n",
    "\n",
    "        if print_out: print('max-depth ' + str(i) + ': '+ str(out['accuracy']))\n",
    "        depth[ind,0] = i\n",
    "        depth[ind,1] = out['accuracy']\n",
    "        ind+=1\n",
    "    \n",
    "    best_depth = depth[np.argmax(depth[:,1]),:]\n",
    "\n",
    "    MAXIMUM_DEPTH = 10000\n",
    "    \n",
    "    #1:200:20   proportion of observations to be eligible to split\n",
    "    min_size = np.c_[np.zeros(10),np.zeros(10)]\n",
    "    proportions = [0.01,0.04,0.07,0.10,0.13,0.16,0.19,0.21,0.24,0.27]\n",
    "    num_obs = dataset.shape[0]\n",
    "    ind=0\n",
    "    for p in proportions:\n",
    "        MINIMAL_SIZE_FOR_SPLIT = p * num_obs\n",
    "        out = random_forest(dataset, attributes, num_folds=NUM_FOLDS, ntree=ntree)\n",
    "        \n",
    "        if print_out: print('min-size ' + str(p) + ': '+ str(out['accuracy']))\n",
    "        min_size[ind,0] = p\n",
    "        min_size[ind,1] = out['accuracy']\n",
    "        ind+=1\n",
    "    \n",
    "    best_min_size = min_size[np.argmax(min_size[:,1]),:]\n",
    "    MINIMAL_SIZE_FOR_SPLIT = 1\n",
    "\n",
    "\n",
    "    #1%,40%,4%  minimum info gain\n",
    "    min_gain = np.c_[np.zeros(10),np.zeros(10)]\n",
    "    ind=0\n",
    "    proportions = [0.01,0.04,0.07,0.10,0.13,0.16,0.19,0.21,0.24,0.27]\n",
    "    for p in proportions:\n",
    "        MINIMAL_INFO_GAIN = p\n",
    "        out = random_forest(dataset, attributes, num_folds=NUM_FOLDS, ntree=ntree)\n",
    "        \n",
    "        if print_out: print('min-gain ' + str(p) + ': '+ str(out['accuracy']))\n",
    "        min_gain[ind,0] = p\n",
    "        min_gain[ind,1] = out['accuracy']\n",
    "        ind+=1\n",
    "    \n",
    "    best_min_gain = min_gain[np.argmax(min_gain[:,1]),:]\n",
    "    MINIMAL_INFO_GAIN = 0\n",
    "    \n",
    "    #no stopping condition case:\n",
    "    out = random_forest(dataset, attributes, num_folds=NUM_FOLDS, ntree=ntree)\n",
    "    acc = out['accuracy']\n",
    "    if acc >= best_min_size[1] and acc >= best_depth[1] and acc >= best_min_gain[1]:\n",
    "        print('NO OTHER STOPPING CONDITION')\n",
    "        reset_hyperparameters()\n",
    "        return ''\n",
    "    \n",
    "    \n",
    "    if best_min_size[1] >= best_min_gain[1] and best_min_size[1] >= best_depth[1]:\n",
    "        print('CHOSEN HYPER-PARAMETER: MINIMUM_SIZE_FOR_SPLIT')\n",
    "        optimize = best_min_size\n",
    "        MINIMAL_SIZE_FOR_SPLIT = best_min_size[1]\n",
    "    elif best_min_gain[1] >= best_min_size[1] and best_min_gain[1] >= best_depth[1]:\n",
    "        print('CHOSEN HYPER-PARAMETER: MINIMAL_INFO_GAIN')\n",
    "        optimize = best_min_gain\n",
    "        MINIMAL_INFO_GAIN = best_min_gain[1]\n",
    "    else:\n",
    "        print('CHOSEN HYPER-PARAMETER: MAXIMUM_TREE_DEPTH')\n",
    "        optimize = best_depth\n",
    "        MAXIMUM_DEPTH = best_depth[1]\n",
    "    \n",
    "    out = {\n",
    "        'n-tree':ntree,\n",
    "        'min-size':best_min_size[0],\n",
    "        'min-size-acc':best_min_size[1],\n",
    "        'min-gain':best_min_gain[0],\n",
    "        'min-gain-accuracy':best_min_gain[1],\n",
    "        'max-depth':best_depth[0],\n",
    "        'max-depth-accuracy':best_depth[1]\n",
    "    }\n",
    "    \n",
    "    if print_out: print(out)\n",
    "    \n",
    "    return optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_graphs(dataset, title=''):\n",
    "    x = dataset['ntree']\n",
    "    y = dataset.iloc[:,0:4]\n",
    "\n",
    "    plots = []\n",
    "    for col in y.columns:\n",
    "        plt.plot(x,y[col], color='orange')\n",
    "        \n",
    "        peak = dataset['ntree'][np.argmax(dataset[col])]\n",
    "        plt.axvline(x = peak, color = 'black', linestyle='--', label = 'axvline - full height')\n",
    "        \n",
    "        plt.title(title + ' ' + col)\n",
    "        plt.xlabel('Number of Trees (ntree)')\n",
    "        plt.ylabel(col)\n",
    "        \n",
    "        text = 'peak ' + col + ': ' + str(round(np.max(dataset[col]),4)) + ' at ntree=' + str(peak)\n",
    "        \n",
    "        #I had to change this for every dataset by the way\n",
    "        plt.text(20, 0.95, text, fontsize=10, color='red')\n",
    "    \n",
    "        plt.grid()\n",
    "\n",
    "        save_name = 'plots/' + title + '-' + col + '.png'\n",
    "        plt.savefig(save_name)\n",
    "\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "House Votes dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"IMPORTANT: I deleted the first character: '#' in the csv file to make it run\"\"\"\n",
    "# def house_votes():\n",
    "#     house_votes = pd.read_csv('datasets/hw3_house_votes_84.csv',comment='#', dtype='int')\n",
    "\n",
    "#     house_votes_attributes = pd.DataFrame(data=np.c_[house_votes.columns, np.empty(len(house_votes.columns))], columns=['attribute', 'att_type'])\n",
    "\n",
    "#     for i in range(house_votes_attributes.shape[0]):\n",
    "#         house_votes_attributes.loc[i, 'att_type'] = 'categorical'\n",
    "\n",
    "\n",
    "#     optimized_param = optimize_stop_condition(house_votes, house_votes_attributes, ntree=10,print_out=True)\n",
    "#     print(f'stopping condition: {optimized_param}')\n",
    "    \n",
    "#     house_stats=[]\n",
    "#     for ntree in TREE_NUMS:\n",
    "        \n",
    "#         out = random_forest(house_votes, house_votes_attributes, num_folds=NUM_FOLDS, ntree=ntree)\n",
    "        \n",
    "#         out['ntree'] = ntree\n",
    "        \n",
    "#         print('ntree ' + str(ntree) + ' done')\n",
    "#         print('ACCURACY: ' + str(out['accuracy']))\n",
    "        \n",
    "#         house_stats.append(out)\n",
    "        \n",
    "#         print(out)\n",
    "        \n",
    "#     reset_hyperparameters()\n",
    "    \n",
    "#     house_stats = pd.DataFrame(house_stats)\n",
    "#     return house_stats\n",
    "\n",
    "# house_stats = house_votes()\n",
    "# plots = make_graphs(house_stats, title='House-Votes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wine Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"IMPORTANT: I deleted the first two characters: '# ' in the csv file to make it run\"\"\"\n",
    "# def wine():\n",
    "#     global optimize_numerical\n",
    "#     wine = pd.read_csv('datasets/hw3_wine.csv', delim_whitespace=True)\n",
    "\n",
    "#     wine_attributes = pd.DataFrame(data=np.c_[wine.columns, np.empty(len(wine.columns))], columns=['attribute', 'att_type'])\n",
    "\n",
    "#     for i in range(wine_attributes.shape[0]):\n",
    "#         wine_attributes.loc[i, 'att_type'] = 'numerical'\n",
    "\n",
    "#     optimize_numerical = True\n",
    "#     optimized_param = optimize_stop_condition(wine, wine_attributes, ntree=5,print_out=True)\n",
    "#     print(f'stopping condition: {optimized_param}')\n",
    "#     # optimize_numerical = False\n",
    "\n",
    "#     wine_stats=[]\n",
    "#     for ntree in TREE_NUMS:\n",
    "        \n",
    "#         out = random_forest(wine, wine_attributes, num_folds=NUM_FOLDS, ntree=ntree)\n",
    "        \n",
    "#         out['ntree'] = ntree\n",
    "        \n",
    "#         print('ntree ' + str(ntree) + ' done')\n",
    "#         print('ACCURACY: ' + str(out['accuracy']))\n",
    "        \n",
    "#         wine_stats.append(out)\n",
    "        \n",
    "#         print(out)\n",
    "        \n",
    "#     reset_hyperparameters()\n",
    "    \n",
    "#     wine_stats = pd.DataFrame(wine_stats)\n",
    "#     return wine_stats\n",
    "\n",
    "# wine_stats = wine()\n",
    "# make_graphs(wine_stats, title='Wine-dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cancer Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cancer():\n",
    "#     global optimize_numerical\n",
    "#     cancer = pd.read_csv('datasets/hw3_cancer.csv', delim_whitespace=True)\n",
    "#     cancer = cancer.rename(columns={'Class': 'class'}) #i use lowercase to access\n",
    "\n",
    "#     cancer_attributes = pd.DataFrame(data=np.c_[cancer.columns, np.empty(len(cancer.columns))], columns=['attribute', 'att_type'])\n",
    "\n",
    "#     for i in range(cancer_attributes.shape[0]):\n",
    "#         cancer_attributes.loc[i, 'att_type'] = 'numerical'\n",
    "        \n",
    "\n",
    "#     optimize_numerical = True\n",
    "#     optimized_param = optimize_stop_condition(cancer, cancer_attributes, ntree=5,print_out=True)\n",
    "#     print(f'stopping condition: {optimized_param}')\n",
    "#     optimize_numerical = False\n",
    "\n",
    "#     cancer_stats=[]\n",
    "#     for ntree in TREE_NUMS:\n",
    "        \n",
    "#         out = random_forest(cancer, cancer_attributes, num_folds=NUM_FOLDS, ntree=ntree)\n",
    "        \n",
    "#         out['ntree'] = ntree\n",
    "        \n",
    "#         print('ntree ' + str(ntree) + ' done')\n",
    "#         print('ACCURACY: ' + str(out['accuracy']))\n",
    "        \n",
    "#         cancer_stats.append(out)\n",
    "        \n",
    "#         print(out)\n",
    "        \n",
    "#     reset_hyperparameters()\n",
    "    \n",
    "#     cancer_stats = pd.DataFrame(cancer_stats)\n",
    "#     return cancer_stats\n",
    "\n",
    "# cancer_stats = cancer() #9m 50s\n",
    "# make_graphs(cancer_stats, 'Cancer-dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def contraceptive():\n",
    "#     global optimize_numerical\n",
    "#     #preparing categories and types\n",
    "#     contraceptive_attributes = pd.DataFrame(data=np.c_[np.empty(10), np.empty(10)], columns=['attribute', 'att_type'])\n",
    "#     contraceptive_attributes.iloc[0,0] = 'wife-age'\n",
    "#     contraceptive_attributes.iloc[0,1] = 'numerical'\n",
    "\n",
    "#     contraceptive_attributes.iloc[1,0] = 'wife-educ'\n",
    "#     contraceptive_attributes.iloc[1,1] = 'categorical'\n",
    "\n",
    "#     contraceptive_attributes.iloc[2,0] = 'husband-educ'\n",
    "#     contraceptive_attributes.iloc[2,1] = 'categorical'\n",
    "\n",
    "#     contraceptive_attributes.iloc[3,0] = 'num-children'\n",
    "#     contraceptive_attributes.iloc[3,1] = 'numerical'\n",
    "\n",
    "#     contraceptive_attributes.iloc[4,0] = 'wife-religion'\n",
    "#     contraceptive_attributes.iloc[4,1] = 'categorical'\n",
    "\n",
    "#     contraceptive_attributes.iloc[5,0] = 'wife-working'\n",
    "#     contraceptive_attributes.iloc[5,1] = 'categorical'\n",
    "\n",
    "#     contraceptive_attributes.iloc[6,0] = 'husband-job'\n",
    "#     contraceptive_attributes.iloc[6,1] = 'categorical'\n",
    "\n",
    "#     contraceptive_attributes.iloc[7,0] = 'living-index'\n",
    "#     contraceptive_attributes.iloc[7,1] = 'categorical'\n",
    "\n",
    "#     contraceptive_attributes.iloc[8,0] = 'media-exposure'\n",
    "#     contraceptive_attributes.iloc[8,1] = 'categorical'\n",
    "\n",
    "#     contraceptive_attributes.iloc[9,0] = 'class'\n",
    "#     contraceptive_attributes.iloc[9,1] = 'categorical'\n",
    "\n",
    "#     contraceptive = pd.read_csv('datasets/contraceptive+method+choice/cmc.data', \n",
    "#                                 delimiter=',', header=None,\n",
    "#                                 names=contraceptive_attributes.iloc[:,0])\n",
    "    \n",
    "\n",
    "#     optimize_numerical = True\n",
    "#     optimized_param = optimize_stop_condition(contraceptive, contraceptive_attributes, ntree=5,print_out=True)\n",
    "#     print(f'stopping condition: {optimized_param}')\n",
    "#     # optimize_numerical = False\n",
    "\n",
    "#     contraceptive_stats=[]\n",
    "#     for ntree in TREE_NUMS:\n",
    "        \n",
    "#         out = random_forest(contraceptive, contraceptive_attributes, num_folds=NUM_FOLDS, ntree=ntree)\n",
    "        \n",
    "#         out['ntree'] = ntree\n",
    "        \n",
    "#         print('ntree ' + str(ntree) + ' done')\n",
    "#         print('ACCURACY: ' + str(out['accuracy']))\n",
    "        \n",
    "#         contraceptive_stats.append(out)\n",
    "        \n",
    "#         print(out)\n",
    "        \n",
    "#     reset_hyperparameters()\n",
    "    \n",
    "#     contraceptive_stats = pd.DataFrame(contraceptive_stats)\n",
    "#     return contraceptive_stats\n",
    "\n",
    "# contraceptive_stats = contraceptive()\n",
    "# make_graphs(contraceptive_stats, 'Contraceptives')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max-depth 3: 0.7291666666666666\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 40\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# optimize_numerical = True\u001b[39;00m\n\u001b[1;32m     39\u001b[0m optimize_numerical \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m optimized_param \u001b[38;5;241m=\u001b[39m \u001b[43moptimize_stop_condition\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloan_attributes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mntree\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mprint_out\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstopping condition: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moptimized_param\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     43\u001b[0m loan_stats\u001b[38;5;241m=\u001b[39m[]\n",
      "Cell \u001b[0;32mIn[7], line 19\u001b[0m, in \u001b[0;36moptimize_stop_condition\u001b[0;34m(dataset, attributes, ntree, print_out)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m22\u001b[39m,\u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m     18\u001b[0m     MAXIMUM_DEPTH \u001b[38;5;241m=\u001b[39m i\n\u001b[0;32m---> 19\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mrandom_forest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattributes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_folds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUM_FOLDS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mntree\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mntree\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m print_out: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax-depth \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(i) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(out[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[1;32m     22\u001b[0m     depth[ind,\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m i\n",
      "Cell \u001b[0;32mIn[6], line 38\u001b[0m, in \u001b[0;36mrandom_forest\u001b[0;34m(dataset, attribute_list, num_folds, ntree)\u001b[0m\n\u001b[1;32m     34\u001b[0m output \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data\u001b[38;5;241m=\u001b[39my_true\u001b[38;5;241m.\u001b[39mvalues,columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     36\u001b[0m train \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(subsets_copy, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 38\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mrun_forest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattribute_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mntree\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m majority_vote \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(output\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ntree \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "Cell \u001b[0;32mIn[6], line 9\u001b[0m, in \u001b[0;36mrun_forest\u001b[0;34m(train, test, attribute_list, output, ntree)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(ntree):\n\u001b[1;32m      4\u001b[0m     \n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m#Bootstrap Aggregating:\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     attribute_sublist \u001b[38;5;241m=\u001b[39m subset_attributes(attribute_list)\n\u001b[0;32m----> 9\u001b[0m     tree \u001b[38;5;241m=\u001b[39m \u001b[43mdecision_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattribute_sublist\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     y_hat \u001b[38;5;241m=\u001b[39m predict(tree, test)\n\u001b[1;32m     12\u001b[0m     label \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124my_hat\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(t)\n",
      "Cell \u001b[0;32mIn[4], line 48\u001b[0m, in \u001b[0;36mdecision_tree\u001b[0;34m(dataset, attribute_list, depth)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m left_dataset\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     47\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 48\u001b[0m     left_node \u001b[38;5;241m=\u001b[39m \u001b[43mdecision_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleft_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattribute_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(right_dataset) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     51\u001b[0m     right_node \u001b[38;5;241m=\u001b[39m Node(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[4], line 48\u001b[0m, in \u001b[0;36mdecision_tree\u001b[0;34m(dataset, attribute_list, depth)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m left_dataset\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     47\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 48\u001b[0m     left_node \u001b[38;5;241m=\u001b[39m \u001b[43mdecision_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleft_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattribute_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(right_dataset) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     51\u001b[0m     right_node \u001b[38;5;241m=\u001b[39m Node(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[4], line 55\u001b[0m, in \u001b[0;36mdecision_tree\u001b[0;34m(dataset, attribute_list, depth)\u001b[0m\n\u001b[1;32m     53\u001b[0m     right_node\u001b[38;5;241m.\u001b[39mset_as_leaf(decision)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 55\u001b[0m     right_node \u001b[38;5;241m=\u001b[39m \u001b[43mdecision_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mright_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattribute_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m parent\u001b[38;5;241m.\u001b[39mset_left(left_node)\n\u001b[1;32m     59\u001b[0m parent\u001b[38;5;241m.\u001b[39mset_right(right_node)\n",
      "    \u001b[0;31m[... skipping similar frames: decision_tree at line 48 (2 times)]\u001b[0m\n",
      "Cell \u001b[0;32mIn[4], line 55\u001b[0m, in \u001b[0;36mdecision_tree\u001b[0;34m(dataset, attribute_list, depth)\u001b[0m\n\u001b[1;32m     53\u001b[0m     right_node\u001b[38;5;241m.\u001b[39mset_as_leaf(decision)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 55\u001b[0m     right_node \u001b[38;5;241m=\u001b[39m \u001b[43mdecision_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mright_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattribute_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m parent\u001b[38;5;241m.\u001b[39mset_left(left_node)\n\u001b[1;32m     59\u001b[0m parent\u001b[38;5;241m.\u001b[39mset_right(right_node)\n",
      "    \u001b[0;31m[... skipping similar frames: decision_tree at line 55 (3 times), decision_tree at line 48 (2 times)]\u001b[0m\n",
      "Cell \u001b[0;32mIn[4], line 55\u001b[0m, in \u001b[0;36mdecision_tree\u001b[0;34m(dataset, attribute_list, depth)\u001b[0m\n\u001b[1;32m     53\u001b[0m     right_node\u001b[38;5;241m.\u001b[39mset_as_leaf(decision)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 55\u001b[0m     right_node \u001b[38;5;241m=\u001b[39m \u001b[43mdecision_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mright_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattribute_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m parent\u001b[38;5;241m.\u001b[39mset_left(left_node)\n\u001b[1;32m     59\u001b[0m parent\u001b[38;5;241m.\u001b[39mset_right(right_node)\n",
      "Cell \u001b[0;32mIn[4], line 48\u001b[0m, in \u001b[0;36mdecision_tree\u001b[0;34m(dataset, attribute_list, depth)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m left_dataset\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     47\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 48\u001b[0m     left_node \u001b[38;5;241m=\u001b[39m \u001b[43mdecision_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleft_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattribute_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(right_dataset) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     51\u001b[0m     right_node \u001b[38;5;241m=\u001b[39m Node(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[4], line 11\u001b[0m, in \u001b[0;36mdecision_tree\u001b[0;34m(dataset, attribute_list, depth)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m leaf\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#determines best attribute to split on\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m split_attribute \u001b[38;5;241m=\u001b[39m \u001b[43mdetermine_attribute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattribute_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#more stopping criteria based on information gain\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m split_attribute[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minfo_gain\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m MINIMAL_INFO_GAIN: \n",
      "Cell \u001b[0;32mIn[3], line 126\u001b[0m, in \u001b[0;36mdetermine_attribute\u001b[0;34m(dataset, attribute_list)\u001b[0m\n\u001b[1;32m    122\u001b[0m attribute \u001b[38;5;241m=\u001b[39m attribute_list\u001b[38;5;241m.\u001b[39mloc[i, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattribute\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    124\u001b[0m column \u001b[38;5;241m=\u001b[39m dataset[[attribute, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[0;32m--> 126\u001b[0m gain, split_val \u001b[38;5;241m=\u001b[39m \u001b[43mentropy_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattribute_list\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m attribute_list\u001b[38;5;241m.\u001b[39mloc[i, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minfo_gain\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m gain\n\u001b[1;32m    129\u001b[0m attribute_list\u001b[38;5;241m.\u001b[39mloc[i, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msplit_value\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m split_val\n",
      "Cell \u001b[0;32mIn[3], line 64\u001b[0m, in \u001b[0;36mentropy_column\u001b[0;34m(dataset, attribute)\u001b[0m\n\u001b[1;32m     61\u001b[0m left_length \u001b[38;5;241m=\u001b[39m left\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     62\u001b[0m right_length \u001b[38;5;241m=\u001b[39m right\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 64\u001b[0m left_entropy \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleft\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m right_entropy \u001b[38;5;241m=\u001b[39m compute_entropy(right)\n\u001b[1;32m     67\u001b[0m entropy \u001b[38;5;241m=\u001b[39m left_entropy \u001b[38;5;241m*\u001b[39m (left_length\u001b[38;5;241m/\u001b[39mN) \u001b[38;5;241m+\u001b[39m right_entropy \u001b[38;5;241m*\u001b[39m (right_length\u001b[38;5;241m/\u001b[39mN)\n",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m, in \u001b[0;36mcompute_entropy\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m      2\u001b[0m dataset \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      3\u001b[0m N \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m----> 4\u001b[0m class_list \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue_counts\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m entropy \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m count \u001b[38;5;129;01min\u001b[39;00m class_list\u001b[38;5;241m.\u001b[39mvalues:\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/pandas/core/base.py:1010\u001b[0m, in \u001b[0;36mIndexOpsMixin.value_counts\u001b[0;34m(self, normalize, sort, ascending, bins, dropna)\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[1;32m    924\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalue_counts\u001b[39m(\n\u001b[1;32m    925\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    930\u001b[0m     dropna: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    931\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Series:\n\u001b[1;32m    932\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    933\u001b[0m \u001b[38;5;124;03m    Return a Series containing counts of unique values.\u001b[39;00m\n\u001b[1;32m    934\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1008\u001b[0m \u001b[38;5;124;03m    Name: count, dtype: int64\u001b[39;00m\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1010\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue_counts_internal\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1011\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1012\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[43m        \u001b[49m\u001b[43mascending\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mascending\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbins\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/pandas/core/algorithms.py:951\u001b[0m, in \u001b[0;36mvalue_counts_internal\u001b[0;34m(values, sort, ascending, normalize, bins, dropna)\u001b[0m\n\u001b[1;32m    940\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    941\u001b[0m                 \u001b[38;5;66;03m# GH#56161\u001b[39;00m\n\u001b[1;32m    942\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe behavior of value_counts with object-dtype is deprecated. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    947\u001b[0m                 stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    948\u001b[0m             )\n\u001b[1;32m    949\u001b[0m         idx\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m=\u001b[39m index_name\n\u001b[0;32m--> 951\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mSeries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcounts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sort:\n\u001b[1;32m    954\u001b[0m     result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39msort_values(ascending\u001b[38;5;241m=\u001b[39mascending)\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/pandas/core/series.py:588\u001b[0m, in \u001b[0;36mSeries.__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    586\u001b[0m manager \u001b[38;5;241m=\u001b[39m _get_option(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.data_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m, silent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    587\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblock\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 588\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mSingleBlockManager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrefs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrefs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    590\u001b[0m     data \u001b[38;5;241m=\u001b[39m SingleArrayManager\u001b[38;5;241m.\u001b[39mfrom_array(data, index)\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/pandas/core/internals/managers.py:1870\u001b[0m, in \u001b[0;36mSingleBlockManager.from_array\u001b[0;34m(cls, array, index, refs)\u001b[0m\n\u001b[1;32m   1868\u001b[0m array \u001b[38;5;241m=\u001b[39m maybe_coerce_values(array)\n\u001b[1;32m   1869\u001b[0m bp \u001b[38;5;241m=\u001b[39m BlockPlacement(\u001b[38;5;28mslice\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(index)))\n\u001b[0;32m-> 1870\u001b[0m block \u001b[38;5;241m=\u001b[39m \u001b[43mnew_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplacement\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mndim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrefs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrefs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1871\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(block, index)\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/pandas/core/internals/blocks.py:2728\u001b[0m, in \u001b[0;36mnew_block\u001b[0;34m(values, placement, ndim, refs)\u001b[0m\n\u001b[1;32m   2716\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnew_block\u001b[39m(\n\u001b[1;32m   2717\u001b[0m     values,\n\u001b[1;32m   2718\u001b[0m     placement: BlockPlacement,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2725\u001b[0m     \u001b[38;5;66;03m# - check_ndim/ensure_block_shape already checked\u001b[39;00m\n\u001b[1;32m   2726\u001b[0m     \u001b[38;5;66;03m# - maybe_coerce_values already called/unnecessary\u001b[39;00m\n\u001b[1;32m   2727\u001b[0m     klass \u001b[38;5;241m=\u001b[39m get_block_type(values\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m-> 2728\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mklass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mndim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mndim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplacement\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mplacement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrefs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrefs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loan = pd.read_csv('./datasets/loan.csv')\n",
    "\n",
    "loan = loan.drop(columns={'Loan_ID'})\n",
    "\n",
    "loan = loan.rename(columns={'Loan_Status':'class'})\n",
    "\n",
    "#I have to map every single column to numbers\n",
    "loan['class'] = loan['class'].map({'Y': 1, 'N': 0})\n",
    "loan['Gender'] = loan['Gender'].map({'Male': 1, 'Female': 0})\n",
    "loan['Married'] = loan['Married'].map({'Yes': 1, 'No': 0})\n",
    "loan['Dependents'] = loan['Dependents'].map({'3+': 3, '2':2, '1':1, '0':0})\n",
    "loan['Education'] = loan['Education'].map({'Graduate': 1, 'Not Graduate': 0})\n",
    "loan['Self_Employed'] = loan['Self_Employed'].map({'Yes': 1, 'No': 0})\n",
    "loan['Property_Area'] = loan['Property_Area'].map({'Urban': 2, 'Semiurban': 1, 'Rural':0})\n",
    "\n",
    "loan_attributes = pd.DataFrame(data=np.c_[loan.columns, np.empty(12)],\n",
    "                               columns=['attribute', 'att_type'])\n",
    "\n",
    "\n",
    "\n",
    "loan_attributes.iloc[0,1] = 'categorical'\n",
    "loan_attributes.iloc[1,1] = 'categorical'\n",
    "loan_attributes.iloc[2,1] = 'categorical'\n",
    "loan_attributes.iloc[3,1] = 'categorical'\n",
    "loan_attributes.iloc[4,1] = 'categorical'\n",
    "loan_attributes.iloc[5,1] = 'numerical'\n",
    "loan_attributes.iloc[6,1] = 'numerical'\n",
    "loan_attributes.iloc[7,1] = 'numerical'\n",
    "loan_attributes.iloc[8,1] = 'numerical'\n",
    "loan_attributes.iloc[9,1] = 'categorical'\n",
    "loan_attributes.iloc[10,1] = 'categorical'\n",
    "loan_attributes.iloc[11,1] = 'categorical'\n",
    "\n",
    "\n",
    "\n",
    "stats = random_forest(loan, loan_attributes, num_folds=10, ntree=1)\n",
    "\n",
    "optimize_numerical = True\n",
    "# optimize_numerical = False\n",
    "optimized_param = optimize_stop_condition(loan, loan_attributes, ntree=5,print_out=True)\n",
    "print(f'stopping condition: {optimized_param}')\n",
    "\n",
    "loan_stats=[]\n",
    "for ntree in TREE_NUMS:\n",
    "\n",
    "    out = random_forest(loan, loan_attributes, num_folds=NUM_FOLDS, ntree=ntree)\n",
    "\n",
    "    out['ntree'] = ntree\n",
    "\n",
    "    print('ntree ' + str(ntree) + ' done')\n",
    "    print('ACCURACY: ' + str(out['accuracy']))\n",
    "\n",
    "    loan_stats.append(out)\n",
    "\n",
    "    print(out)\n",
    "\n",
    "reset_hyperparameters()\n",
    "    \n",
    "loan_stats = pd.DataFrame(loan_stats)\n",
    "# return loan_stats\n",
    "\n",
    "# contraceptive_stats = contraceptive()\n",
    "make_graphs(loan_stats, 'Loan')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
