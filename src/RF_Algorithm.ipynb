{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries and Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# import warnings\n",
    "# with warnings.catch_warnings():\n",
    "#     warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "\"\"\"Default Global Values for splitting criteria in decision trees\"\"\"\n",
    "MINIMAL_SIZE_FOR_SPLIT = 1\n",
    "\n",
    "MAXIMUM_DEPTH = 1000\n",
    "\n",
    "MINIMAL_INFO_GAIN = 0\n",
    "\n",
    "optimize_numerical = False\n",
    "\n",
    "random.seed(1)\n",
    "TREE_NUMS = [1,2,5,10,20,30,50]\n",
    "NUM_FOLDS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree ADT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    #building the tree\n",
    "    def __init__(self, is_numeric):\n",
    "        self.isLeaf = False\n",
    "        self.numeric = is_numeric\n",
    "        \n",
    "    def set_attribute(self, attribute, split_value):\n",
    "        self.attribute = attribute\n",
    "        self.split_value = split_value\n",
    "        \n",
    "    def set_as_leaf(self, decision):\n",
    "        self.isLeaf = True\n",
    "        self.decision = decision\n",
    "        \n",
    "    def set_left(self, node):\n",
    "        self.left = node\n",
    "        \n",
    "    def set_right(self, node):\n",
    "        self.right = node\n",
    "        \n",
    "    #making a decision\n",
    "    def get_attribute(self):\n",
    "        return self.attribute\n",
    "    def get_split_value(self):\n",
    "        return self.split_value\n",
    "    def is_numeric(self):\n",
    "        return self.numeric\n",
    "    \n",
    "    def get_left(self):\n",
    "        return self.left\n",
    "    def get_right(self):\n",
    "        return self.right\n",
    "    \n",
    "    def is_leaf(self):\n",
    "        return self.isLeaf\n",
    "    def get_decision(self):\n",
    "        return self.decision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree mechanics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_entropy(dataset):\n",
    "    dataset = dataset['class']\n",
    "    N = dataset.shape[0]\n",
    "    class_list = dataset.value_counts()\n",
    "\n",
    "    entropy = 0\n",
    "    for count in class_list.values:\n",
    "        prob = count / N\n",
    "        log_prob = np.log2(prob)\n",
    "        entropy -= prob * log_prob\n",
    "    return entropy\n",
    "\n",
    "def split_dataset(dataset, numerical, attribute, split_val):\n",
    "    if numerical:\n",
    "        left = dataset[dataset[attribute] < split_val]\n",
    "        right = dataset[dataset[attribute] >= split_val]\n",
    "        return left, right\n",
    "    else:\n",
    "        right = dataset.loc[dataset[attribute] == split_val]\n",
    "        left = dataset.loc[dataset[attribute] != split_val]\n",
    "        return left, right\n",
    "\n",
    "def entropy_column(dataset, attribute):\n",
    "    att_name = attribute.iloc[0]\n",
    "    data_type = attribute.iloc[1]\n",
    "    N = dataset.shape[0]\n",
    "    \n",
    "    \n",
    "\n",
    "    if data_type == 'numerical':\n",
    "        if optimize_numerical:\n",
    "            split = np.mean(dataset.iloc[:,0])\n",
    "            \n",
    "            left, right = split_dataset(dataset, numerical=True, attribute=att_name, split_val=split)\n",
    "            left_length = left.shape[0]\n",
    "            right_length = right.shape[0]\n",
    "            \n",
    "            left_entropy = compute_entropy(left)\n",
    "            right_entropy = compute_entropy(right)\n",
    "\n",
    "            entropy = left_entropy * (left_length/N) + right_entropy * (right_length/N)\n",
    "            return entropy, split\n",
    "        \n",
    "        #this method sorts the column by ascending unique values, takes the average of two consecutive values,\n",
    "        #   and performs the split on the resulting value. Then the one with the lowest resulting entropy is chosen\n",
    "        #   and returns both the entropy and split val\n",
    "        dataset = dataset.sort_values(by=dataset.columns[0])\n",
    "        \n",
    "        #gets unique values and creates information tracker\n",
    "        att_values = dataset.iloc[:, 0].unique()\n",
    "        split_entropy = np.c_[np.zeros(len(att_values) - 1), np.zeros(len(att_values) - 1)]\n",
    "        \n",
    "        \n",
    "        for i in range(len(att_values)-1):\n",
    "            split = (att_values[i] + att_values[i+1])/2\n",
    "            split_entropy[i, 0] = split\n",
    "            \n",
    "            #splits dataset above and below split\n",
    "            left, right = split_dataset(dataset, numerical=True, attribute=att_name, split_val=split)\n",
    "            \n",
    "            left_length = left.shape[0]\n",
    "            right_length = right.shape[0]\n",
    "            \n",
    "            left_entropy = compute_entropy(left)\n",
    "            right_entropy = compute_entropy(right)\n",
    "\n",
    "            entropy = left_entropy * (left_length/N) + right_entropy * (right_length/N)\n",
    "            split_entropy[i,1] = entropy\n",
    "        \n",
    "        if split_entropy.shape[0] == 0:\n",
    "            return 0, None\n",
    "        if split_entropy.shape[0] == 1:\n",
    "            return split_entropy[0,1], split_entropy[0,0]\n",
    "        \n",
    "        min_entropy_index = np.argmin(split_entropy[:,1])\n",
    "        \n",
    "        #first item returned is the entropy, second is the value we choose to split at\n",
    "        return split_entropy[min_entropy_index,1], split_entropy[min_entropy_index, 0]\n",
    "    \n",
    "    else:\n",
    "        assert data_type == 'categorical'\n",
    "        \n",
    "        category_values = dataset[att_name].value_counts()\n",
    "        categories = np.transpose(category_values.axes)\n",
    "        \n",
    "        split_entropy = pd.DataFrame(data=np.c_[categories, np.zeros(categories.shape[0])], columns=['category', 'entropy'])\n",
    "        categories = categories.flatten()\n",
    "        \n",
    "        for cat in categories:\n",
    "            part_one, part_two = split_dataset(dataset, numerical=False, attribute=att_name, split_val=cat)\n",
    "            \n",
    "            part_one_entropy = compute_entropy(part_one)\n",
    "            part_two_entropy = compute_entropy(part_two)\n",
    "            \n",
    "            n = category_values[cat] #num times cat appears in dataset\n",
    "            entropy = part_one_entropy * n / N + part_two_entropy * (N-n) / N #weighed avg\n",
    "            split_entropy.loc[split_entropy['category'] == cat, 'entropy'] = entropy\n",
    "        \n",
    "        min_row = np.argmin(split_entropy['entropy'])\n",
    "        \n",
    "        return split_entropy.loc[min_row, 'entropy'], split_entropy.loc[min_row, 'category']\n",
    "\n",
    "def determine_attribute(dataset, attribute_list):\n",
    "    attribute_list = attribute_list.copy()\n",
    "    \n",
    "    presplit_entropy = compute_entropy(dataset)\n",
    "    \n",
    "    #sets up information for debugging and decision\n",
    "    #attribute_list has columns 'attribute', 'attribute type', 'information gain', and 'split_value\n",
    "    info_gain = np.ones(attribute_list.shape[0]) * presplit_entropy #stores info_gain\n",
    "    \n",
    "    #this column either contains the numerical split value or class to separate\n",
    "    split_value = np.zeros(attribute_list.shape[0])\n",
    "    \n",
    "    attribute_list.insert(2, 'info_gain', info_gain)\n",
    "    attribute_list.insert(3, 'split_value', split_value)\n",
    "    attribute_list = attribute_list.drop(attribute_list[attribute_list['attribute'] == 'class'].index)\n",
    "    attribute_list = attribute_list.reset_index(drop=True)\n",
    "    \n",
    "    #computes info gain for each column\n",
    "    for i in range(len(attribute_list)):\n",
    "        attribute = attribute_list.loc[i, 'attribute']\n",
    "        \n",
    "        column = dataset[[attribute, 'class']]\n",
    "        \n",
    "        gain, split_val = entropy_column(column, attribute_list.iloc[i])\n",
    "        \n",
    "        attribute_list.loc[i, 'info_gain'] -= gain\n",
    "        attribute_list.loc[i, 'split_value'] = split_val\n",
    "    \n",
    "    #sorts and chooses attribute with highest info gain\n",
    "    attribute_list = attribute_list.sort_values(by='info_gain', ascending=False)\n",
    "    return attribute_list.iloc[0]\n",
    "\n",
    "def get_majority_class(dataset):\n",
    "    dataset = dataset['class']\n",
    "    highest_class = dataset.value_counts().index[0]\n",
    "    return highest_class\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_tree(dataset, attribute_list, depth=0):\n",
    "    #stopping criteria:\n",
    "    curr_entropy = compute_entropy(dataset)\n",
    "    if (curr_entropy == 0 or attribute_list.shape[0] <= MINIMAL_SIZE_FOR_SPLIT or depth >= MAXIMUM_DEPTH): \n",
    "        leaf = Node(False)\n",
    "        decision = get_majority_class(dataset)\n",
    "        leaf.set_as_leaf(decision)\n",
    "        return leaf\n",
    "    \n",
    "    #determines best attribute to split on\n",
    "    split_attribute = determine_attribute(dataset, attribute_list)\n",
    "    \n",
    "    #more stopping criteria based on information gain\n",
    "    if split_attribute['info_gain'] <= MINIMAL_INFO_GAIN: \n",
    "        leaf = Node(False)\n",
    "        decision = get_majority_class(dataset)\n",
    "        leaf.set_as_leaf(decision)\n",
    "        return leaf\n",
    "    \n",
    "    #splits dataset based on the value of the attribute\n",
    "    #   for categorical: right node if attribute equals split value else left node\n",
    "    #   for numerical: right node if attribute greater than or equal to split value else left node\n",
    "    numerical = True if split_attribute['att_type'] == 'numerical' else False\n",
    "    \n",
    "    parent = Node(is_numeric=numerical)\n",
    "    parent.set_attribute(split_attribute['attribute'], split_attribute['split_value'])\n",
    "    \n",
    "    left_dataset, right_dataset = split_dataset(dataset, numerical, \n",
    "                                                split_attribute['attribute'], \n",
    "                                                split_val=split_attribute['split_value'])\n",
    "    \n",
    "    #removes category once we split on it\n",
    "    if not numerical:\n",
    "        attribute_name = split_attribute['attribute']\n",
    "        attribute_list = attribute_list[attribute_list['attribute'] != attribute_name]\n",
    "        \n",
    "        parent.set_attribute(split_attribute['attribute'], split_attribute['split_value'])\n",
    "    \n",
    "    \n",
    "    #recursively creates child nodes\n",
    "    if len(left_dataset) == 0: #set to majority class in dataset if split results in complete decision\n",
    "        left_node = Node(False)\n",
    "        decision = get_majority_class(dataset)\n",
    "        left_node.set_as_leaf(decision)\n",
    "    else:\n",
    "        if left_dataset.size == 0:\n",
    "            print('error')\n",
    "        left_node = decision_tree(left_dataset, attribute_list, depth=(depth+1))\n",
    "\n",
    "    if len(right_dataset) == 0:\n",
    "        right_node = Node(False)\n",
    "        decision = get_majority_class(dataset)\n",
    "        right_node.set_as_leaf(decision)\n",
    "    else:\n",
    "        right_node = decision_tree(right_dataset, attribute_list, depth=(depth+1))\n",
    "    \n",
    "    \n",
    "    parent.set_left(left_node)\n",
    "    parent.set_right(right_node)\n",
    "    \n",
    "    return parent\n",
    "\n",
    "def traverse_tree(tree, observation):\n",
    "    while not tree.is_leaf():\n",
    "        curr_att = tree.get_attribute()\n",
    "        is_numeric = tree.is_numeric()\n",
    "        split_val = tree.get_split_value()\n",
    "        x = observation[curr_att]\n",
    "        if is_numeric:\n",
    "            if x >= split_val:\n",
    "                tree = tree.get_right()\n",
    "            else:\n",
    "                tree = tree.get_left()\n",
    "        else:\n",
    "            if x == split_val:\n",
    "                tree = tree.get_right()\n",
    "            else:\n",
    "                tree = tree.get_left()\n",
    "    decision = tree.get_decision()\n",
    "    return decision\n",
    "\n",
    "def predict(tree, dataset):\n",
    "    #y_hat and y_true\n",
    "    y_hat = np.empty(dataset.shape[0], dtype='int')\n",
    "    \n",
    "    for i in range(dataset.shape[0]):\n",
    "        observation = dataset.iloc[i]\n",
    "        y_hat[i] = traverse_tree(tree, observation)\n",
    "    \n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing cross-validation, metrics, and bootstrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratify_dataset(dataset, folds):\n",
    "    classes = dataset['class'].value_counts()\n",
    "    class_info = pd.DataFrame({'class': classes.index, 'count': classes.values})\n",
    "    classes = list(classes.axes)\n",
    "    class_info['fold_size'] = np.floor(class_info['count'] / folds)\n",
    "    class_info['remainder'] = class_info['count'] % folds\n",
    "\n",
    "    dataset = dataset.sample(frac=1)\n",
    "    subsets = []\n",
    "    for fold in range(folds):\n",
    "        new_fold = pd.DataFrame(columns=dataset.columns)\n",
    "        for c in class_info['class']:\n",
    "            fold_size = int(class_info.loc[class_info['class'] == c, 'fold_size'].iloc[0])\n",
    "            if class_info.loc[class_info['class'] == c, 'remainder'].iloc[0] > 0:\n",
    "                class_info.loc[class_info['class'] == c, 'remainder'] -= 1\n",
    "                fold_size += 1\n",
    "            \n",
    "            observations = dataset.loc[dataset['class'] == c].head(fold_size)\n",
    "            new_fold = pd.concat([new_fold, observations], ignore_index=True)\n",
    "            # new_fold = new_fold.append(observations, ignore_index=True)\n",
    "            dataset = dataset.drop(observations.index)\n",
    "        new_fold = new_fold.sample(frac=1)\n",
    "        subsets.append(new_fold)\n",
    "    \n",
    "    return subsets\n",
    "\n",
    "def make_statistics(output):\n",
    "    n = output.shape[0]\n",
    "    \n",
    "    #makes df counting confusion matrix values:\n",
    "    classes = list(output['y_true'].value_counts().index)\n",
    "    num_classes = len(classes)\n",
    "    \n",
    "    true_pos = np.zeros(num_classes)\n",
    "    false_pos = np.zeros(num_classes)\n",
    "    true_neg = np.zeros(num_classes)\n",
    "    false_neg = np.zeros(num_classes)\n",
    "    \n",
    "    data = {\n",
    "    'class': classes,\n",
    "    'true_pos': true_pos,\n",
    "    'true_neg': true_neg,\n",
    "    'false_pos': false_pos,\n",
    "    'false_neg': false_neg\n",
    "    }\n",
    "    \n",
    "    confusion_matrix = pd.DataFrame(data, dtype='int')\n",
    "    \n",
    "    #classifies prediction by true and false pos and neg\n",
    "    for i in range(n):\n",
    "        y_true = output.loc[i, 'y_true']\n",
    "        y_hat = output.loc[i, 'y_hat_majority']\n",
    "        class_row = confusion_matrix.index[confusion_matrix['class'] == y_true][0]\n",
    "        \n",
    "        if y_hat == y_true:\n",
    "            #adds true positive for y_true\n",
    "            confusion_matrix.loc[class_row, 'true_pos'] += 1\n",
    "            \n",
    "            #adds true negatives for every class except y_true\n",
    "            for j in range(num_classes):\n",
    "                if j == class_row:\n",
    "                    continue\n",
    "                else:\n",
    "                    confusion_matrix.loc[j, 'true_neg'] += 1\n",
    "        else: #prediction is different from true class\n",
    "            \n",
    "            #adds a false negative for y_true\n",
    "            confusion_matrix.loc[class_row, 'false_neg'] += 1\n",
    "            \n",
    "            #adds false positives for every other class\n",
    "            for j in range(num_classes):\n",
    "                if j == class_row:\n",
    "                    continue\n",
    "                else:\n",
    "                    confusion_matrix.loc[j, 'false_pos'] += 1\n",
    "    \n",
    "    accuracy = confusion_matrix['true_pos'].sum() / n\n",
    "\n",
    "    precision = []\n",
    "    recall = []\n",
    "    for c in range(num_classes):\n",
    "        tp = confusion_matrix.loc[c, 'true_pos']\n",
    "        fp = confusion_matrix.loc[c, 'false_pos']\n",
    "        fn = confusion_matrix.loc[c, 'false_neg']\n",
    "        \n",
    "        precision_c = tp / (tp + fp)\n",
    "        recall_c = tp / (tp + fn)\n",
    "        \n",
    "        precision.append(precision_c)\n",
    "        recall.append(recall_c)\n",
    "        \n",
    "    precision = np.mean(precision)\n",
    "    recall = np.mean(recall)\n",
    "\n",
    "    beta = 1\n",
    "    f1_score = (1 + np.square(beta)) * (precision * recall) / (np.square(beta) * precision + recall)\n",
    "    \n",
    "    statistics = {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1-score': f1_score\n",
    "    }\n",
    "    return statistics\n",
    "\n",
    "def subset_attributes(attribute_list):\n",
    "    \n",
    "    #samples ~a^(1/2) atributes (with replacement) to train subtree\n",
    "    num_attributes = attribute_list.shape[0]\n",
    "    num_subcolumns = int(np.floor(np.sqrt(num_attributes)) + 1)\n",
    "    \n",
    "    attribute_sublist = np.empty(shape=(num_subcolumns,2), dtype='<U100')\n",
    "    \n",
    "    for i in range(num_subcolumns):\n",
    "        r = random.randint(0, (num_attributes - 1)) #inclusive\n",
    "        \n",
    "        att = attribute_list.loc[r,'attribute']\n",
    "        att_type = attribute_list.loc[r,'att_type']\n",
    "        \n",
    "        while att == 'class':\n",
    "            r = random.randint(0, (num_attributes - 1))\n",
    "            att = attribute_list.loc[r,'attribute']\n",
    "            att_type = attribute_list.loc[r,'att_type']\n",
    "        \n",
    "        attribute_sublist[i] = [att, att_type]\n",
    "    \n",
    "    attribute_sublist = pd.DataFrame(data=attribute_sublist, columns=attribute_list.columns)\n",
    "    \n",
    "    return attribute_sublist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compiles Decision trees into forrests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_forest(train, test, attribute_list, output, ntree):\n",
    "    #creates ntree voters which classify the test set\n",
    "    for t in range(ntree):\n",
    "        \n",
    "        #Bootstrap Aggregating:\n",
    "        \n",
    "        attribute_sublist = subset_attributes(attribute_list)\n",
    "        \n",
    "        tree = decision_tree(train, attribute_sublist)\n",
    "        \n",
    "        y_hat = predict(tree, test)\n",
    "        label = 'y_hat' + str(t)\n",
    "        output[label] = y_hat\n",
    "    \n",
    "    return output\n",
    "    \n",
    "    \n",
    "def random_forest(dataset, attribute_list, num_folds, ntree):\n",
    "    dataset = dataset.sample(frac=1) #shuffles dataset\n",
    "    \n",
    "    subsets = stratify_dataset(dataset, num_folds)\n",
    "\n",
    "    #prepares output for statistics\n",
    "    columns = ['y_true', 'y_hat_majority']\n",
    "    output_total = pd.DataFrame(columns=columns)\n",
    "    \n",
    "    #k-folding done here:\n",
    "    for k in range(num_folds):\n",
    "        subsets_copy = subsets.copy()\n",
    "        \n",
    "        test = subsets_copy.pop(k)\n",
    "\n",
    "        y_true = test['class']\n",
    "        output = pd.DataFrame(data=y_true.values,columns=['y_true'])\n",
    "        \n",
    "        train = pd.concat(subsets_copy, ignore_index=True)\n",
    "        \n",
    "        output = run_forest(train,test, attribute_list, output, ntree)\n",
    "        \n",
    "        majority_vote = np.zeros(output.shape[0])\n",
    "        if ntree > 1:\n",
    "            majority_vote = [output.iloc[row, 1:ntree].mode().iloc[0] for row in range(output.shape[0])]\n",
    "        else:\n",
    "            majority_vote = output['y_hat0']\n",
    "        \n",
    "        output = pd.DataFrame(data={'y_true': output['y_true'], 'y_hat_majority': majority_vote})\n",
    "        output_total = pd.concat([output_total, output], ignore_index=True)\n",
    "    \n",
    "    statistics = make_statistics(output_total)\n",
    "    return statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizing the stop condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_hyperparameters():\n",
    "    MAXIMUM_DEPTH = 10000\n",
    "    MINIMAL_SIZE_FOR_SPLIT = 1\n",
    "    MINIMAL_SIZE_FOR_SPLIT = 1\n",
    "    return\n",
    "    \n",
    "def optimize_stop_condition(dataset, attributes, ntree=5, print_out=False):\n",
    "    #sorry for this code, i got really lazy here because of a bug somewhere else\n",
    "    \n",
    "    #MINIMAL_SIZE_FOR_SPLIT, MAXIMUM_DEPTH, MINIMAL_INFO_GAIN\n",
    "    #default: 0, really big, 0\n",
    "    reset_hyperparameters()\n",
    "\n",
    "    #1:20:2     DEPTH\n",
    "    depth = np.c_[np.zeros(10),np.zeros(10)]\n",
    "    ind=0\n",
    "    for i in range(3,22,2):\n",
    "        MAXIMUM_DEPTH = i\n",
    "        out = random_forest(dataset, attributes, num_folds=NUM_FOLDS, ntree=ntree)\n",
    "\n",
    "        if print_out: print('max-depth ' + str(i) + ': '+ str(out['accuracy']))\n",
    "        depth[ind,0] = i\n",
    "        depth[ind,1] = out['accuracy']\n",
    "        ind+=1\n",
    "    \n",
    "    best_depth = depth[np.argmax(depth[:,1]),:]\n",
    "\n",
    "    MAXIMUM_DEPTH = 10000\n",
    "    \n",
    "    #1:200:20   proportion of observations to be eligible to split\n",
    "    min_size = np.c_[np.zeros(10),np.zeros(10)]\n",
    "    proportions = [0.01,0.04,0.07,0.10,0.13,0.16,0.19,0.21,0.24,0.27]\n",
    "    num_obs = dataset.shape[0]\n",
    "    ind=0\n",
    "    for p in proportions:\n",
    "        MINIMAL_SIZE_FOR_SPLIT = p * num_obs\n",
    "        out = random_forest(dataset, attributes, num_folds=NUM_FOLDS, ntree=ntree)\n",
    "        \n",
    "        if print_out: print('min-size ' + str(p) + ': '+ str(out['accuracy']))\n",
    "        min_size[ind,0] = p\n",
    "        min_size[ind,1] = out['accuracy']\n",
    "        ind+=1\n",
    "    \n",
    "    best_min_size = min_size[np.argmax(min_size[:,1]),:]\n",
    "    MINIMAL_SIZE_FOR_SPLIT = 1\n",
    "\n",
    "\n",
    "    #1%,40%,4%  minimum info gain\n",
    "    min_gain = np.c_[np.zeros(10),np.zeros(10)]\n",
    "    ind=0\n",
    "    proportions = [0.01,0.04,0.07,0.10,0.13,0.16,0.19,0.21,0.24,0.27]\n",
    "    for p in proportions:\n",
    "        MINIMAL_INFO_GAIN = p\n",
    "        out = random_forest(dataset, attributes, num_folds=NUM_FOLDS, ntree=ntree)\n",
    "        \n",
    "        if print_out: print('min-gain ' + str(p) + ': '+ str(out['accuracy']))\n",
    "        min_gain[ind,0] = p\n",
    "        min_gain[ind,1] = out['accuracy']\n",
    "        ind+=1\n",
    "    \n",
    "    best_min_gain = min_gain[np.argmax(min_gain[:,1]),:]\n",
    "    MINIMAL_INFO_GAIN = 0\n",
    "    \n",
    "    #no stopping condition case:\n",
    "    out = random_forest(dataset, attributes, num_folds=NUM_FOLDS, ntree=ntree)\n",
    "    acc = out['accuracy']\n",
    "    if acc >= best_min_size[1] and acc >= best_depth[1] and acc >= best_min_gain[1]:\n",
    "        print('NO OTHER STOPPING CONDITION')\n",
    "        reset_hyperparameters()\n",
    "        return ''\n",
    "    \n",
    "    \n",
    "    if best_min_size[1] >= best_min_gain[1] and best_min_size[1] >= best_depth[1]:\n",
    "        print('CHOSEN HYPER-PARAMETER: MINIMUM_SIZE_FOR_SPLIT')\n",
    "        optimize = best_min_size\n",
    "        MINIMAL_SIZE_FOR_SPLIT = best_min_size[1]\n",
    "    elif best_min_gain[1] >= best_min_size[1] and best_min_gain[1] >= best_depth[1]:\n",
    "        print('CHOSEN HYPER-PARAMETER: MINIMAL_INFO_GAIN')\n",
    "        optimize = best_min_gain\n",
    "        MINIMAL_INFO_GAIN = best_min_gain[1]\n",
    "    else:\n",
    "        print('CHOSEN HYPER-PARAMETER: MAXIMUM_TREE_DEPTH')\n",
    "        optimize = best_depth\n",
    "        MAXIMUM_DEPTH = best_depth[1]\n",
    "    \n",
    "    out = {\n",
    "        'n-tree':ntree,\n",
    "        'min-size':best_min_size[0],\n",
    "        'min-size-acc':best_min_size[1],\n",
    "        'min-gain':best_min_gain[0],\n",
    "        'min-gain-accuracy':best_min_gain[1],\n",
    "        'max-depth':best_depth[0],\n",
    "        'max-depth-accuracy':best_depth[1]\n",
    "    }\n",
    "    \n",
    "    if print_out: print(out)\n",
    "    \n",
    "    return optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_graphs(dataset, title=''):\n",
    "    x = dataset['ntree']\n",
    "    y = dataset.iloc[:,0:4]\n",
    "\n",
    "    plots = []\n",
    "    for col in y.columns:\n",
    "        plt.plot(x,y[col], color='orange')\n",
    "        \n",
    "        peak = dataset['ntree'][np.argmax(dataset[col])]\n",
    "        plt.axvline(x = peak, color = 'black', linestyle='--', label = 'axvline - full height')\n",
    "        \n",
    "        plt.title(title + ' ' + col)\n",
    "        plt.xlabel('Number of Trees (ntree)')\n",
    "        plt.ylabel(col)\n",
    "        \n",
    "        text = 'peak ' + col + ': ' + str(round(np.max(dataset[col]),4)) + ' at ntree=' + str(peak)\n",
    "        \n",
    "        #I had to change this for every dataset by the way\n",
    "        plt.text(25, 0.65, text, fontsize=10, color='red')\n",
    "    \n",
    "        plt.grid()\n",
    "\n",
    "        save_name = 'figures/' + title + '-' + col + '.png'\n",
    "        plt.savefig(save_name)\n",
    "\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "332"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "loan = pd.read_csv('./datasets/loan.csv')\n",
    "\n",
    "loan = loan.drop(columns={'Loan_ID'})\n",
    "\n",
    "loan = loan.rename(columns={'Loan_Status':'class'})\n",
    "\n",
    "#I have to map every single column to numbers\n",
    "loan['class'] = loan['class'].map({'Y': 1, 'N': 0})\n",
    "loan['Gender'] = loan['Gender'].map({'Male': 1, 'Female': 0})\n",
    "loan['Married'] = loan['Married'].map({'Yes': 1, 'No': 0})\n",
    "loan['Dependents'] = loan['Dependents'].map({'3+': 3, '2':2, '1':1, '0':0})\n",
    "loan['Education'] = loan['Education'].map({'Graduate': 1, 'Not Graduate': 0})\n",
    "loan['Self_Employed'] = loan['Self_Employed'].map({'Yes': 1, 'No': 0})\n",
    "loan['Property_Area'] = loan['Property_Area'].map({'Urban': 2, 'Semiurban': 1, 'Rural':0})\n",
    "\n",
    "loan_attributes = pd.DataFrame(data=np.c_[loan.columns, np.empty(12)],\n",
    "                               columns=['attribute', 'att_type'])\n",
    "\n",
    "\n",
    "\n",
    "loan_attributes.iloc[0,1] = 'categorical'\n",
    "loan_attributes.iloc[1,1] = 'categorical'\n",
    "loan_attributes.iloc[2,1] = 'categorical'\n",
    "loan_attributes.iloc[3,1] = 'categorical'\n",
    "loan_attributes.iloc[4,1] = 'categorical'\n",
    "loan_attributes.iloc[5,1] = 'numerical'\n",
    "loan_attributes.iloc[6,1] = 'numerical'\n",
    "loan_attributes.iloc[7,1] = 'numerical'\n",
    "loan_attributes.iloc[8,1] = 'numerical'\n",
    "loan_attributes.iloc[9,1] = 'categorical'\n",
    "loan_attributes.iloc[10,1] = 'categorical'\n",
    "loan_attributes.iloc[11,1] = 'categorical'\n",
    "\n",
    "np.sum(loan['class'])\n",
    "\n",
    "# stats = random_forest(loan, loan_attributes, num_folds=10, ntree=1)\n",
    "\n",
    "# optimize_numerical = True\n",
    "# # optimize_numerical = False\n",
    "# optimized_param = optimize_stop_condition(loan, loan_attributes, ntree=5,print_out=True)\n",
    "# print(f'stopping condition: {optimized_param}')\n",
    "\n",
    "# loan_stats=[]\n",
    "# for ntree in TREE_NUMS:\n",
    "\n",
    "#     out = random_forest(loan, loan_attributes, num_folds=NUM_FOLDS, ntree=ntree)\n",
    "\n",
    "#     out['ntree'] = ntree\n",
    "\n",
    "#     print('ntree ' + str(ntree) + ' done')\n",
    "#     print('ACCURACY: ' + str(out['accuracy']))\n",
    "\n",
    "#     loan_stats.append(out)\n",
    "\n",
    "#     print(out)\n",
    "\n",
    "# reset_hyperparameters()\n",
    "    \n",
    "# loan_stats = pd.DataFrame(loan_stats)\n",
    "# return loan_stats\n",
    "\n",
    "# contraceptive_stats = contraceptive()\n",
    "# make_graphs(loan_stats, 'Loan')\n",
    "# loan_stats.to_latex(float_format=\"%.4f\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
